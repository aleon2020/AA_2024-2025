\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{verbatim}
\usepackage{psfrag}
\usepackage{here} 
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={150mm,257mm},
 left=30mm,
 top=20mm,
 }


%\renewcommand\sfdefault{phv}     %use helvetica for sans serif
\renewcommand{\familydefault}{\sfdefault}
\renewcommand{\familydefault}{cmss}

\begin{document}

\begin{center}
\bf{\large 
{\Large Aprendizaje Autom\'atico}\\
{\vspace{0.1cm}}
Pr\'actica 1
}
\end{center}

%\section*{An\'alisis exploratorio de los datos}

\begin{itemize}

\item[1)]  {\bf An\'alisis exploratorio de los datos}

Importa los datos del fichero \texttt{dataset.csv} y realiza el an\'alisis exploratorio de los datos. Describe en el informe los resultados de este análisis y deposita el c\'odigo Python en Aula Virtual en el fichero answer1.ipynb

\end{itemize}


\begin{itemize}

\item[2)] {\bf Clasificaci\'on usando todas las caracter\'{\i}sticas de los datos}

Usando todas las caracter\'{\i}sticas, implementa los m\'etodos 

\begin{itemize}

\item
Logistic Regression, 

\item
SVM, y

\item
Random Trees


\end{itemize}

para clasificar los datos. Describe en en informe los par\'ametros usados y los resultados obtenidos con los distintos m\'etodos y deposita el c\'odigo Python en Aula Virtual en el fichero answer2.ipynb

\end{itemize}





\begin{itemize}

\item[3)] {\bf Clasificaci\'on usando 4 caracter\'{\i}sticas de los datos}

Selecciona 4 caracter\'isticas de los datos. Usando estas caracter\'{\i}sticas, implementa los m\'etodos 

\begin{itemize}

\item
Logistic Regression, 

\item
SVM, y

\item
Random Trees


\end{itemize}

para clasificar los datos. Describe en en informe los par\'ametros usados y los resultados obtenidos con los distintos m\'etodos y deposita el c\'odigo Python en Aula Virtual en el fichero answer3.ipynb

\end{itemize}





\begin{itemize}

\item[4)] {\bf Clasificaci\'on usando 2 caracter\'{\i}sticas de los datos}

Selecciona 2 caracter\'isticas de los datos. Usando estas caracter\'{\i}sticas, implementa los m\'etodos 

\begin{itemize}

\item
Logistic Regression, 

\item
SVM, y

\item
Random Trees

\end{itemize}

para clasificar los datos. Describe en en informe los par\'ametros usados y los resultados obtenidos con los distintos m\'etodos y deposita el c\'odigo Python en Aula Virtual en el fichero answer4.ipynb.

\end{itemize}





\bigskip

\noindent
\textcolor{red}{
Redacta un informe detallado respondiendo a cada pregunta en una secci\'on diferente. Motiva cada elecci\'on realizada durante el proceso de dise\~no de los clasificadores y describe cada resultado obtenido. Compara los resultados obtenidos con distintos n\'umeros de caracter\'{\i}sticas. Incluye el c\'odigo Python en el apartado correspondiente del informe. 
} 

% ----- 1. ANALISIS EXPLORATORIO DE LOS DATOS ----- %

\newpage

\section[1]{An\'alisis exploratorio de los datos}

\begin{itemize}

\item[1.1]  {\bf Importacion del dataset}

El primer paso de este analisis consiste en la importacion del dataset, utilizando la libreria pandas para leer el fichero dataset.csv. A partir de ahi, se configura la visualizacion del dataset para poder observar todas las columnas disponibles.

Este paso es importante para obtener una vista inicial del tamaño y las caracteristicas del dataset, ademas de confirmar que los datos han sido leidos correctamente.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
dataset = pd.read_csv("dataset.csv")
pd.set_option('display.max_columns', len(dataset.columns))
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

\end{itemize}

\bigskip

\begin{itemize}

\item[1.2]  {\bf Inspeccion del dataset}

A continuacion, se evalua la estructura del dataset observando las dimensiones (dataset.shape), las primeras filas (dataset.head(1)), la informacion de cada columna y otras estadisticas descriptivas como la media, la desviacion estandar, minimos, maximos y cuartiles, ademas de otros valores atipicos y distribuciones inusuales (dataset.describe()).

Todo esto permite conocer cuantas filas y columnas contiene el dataset (dataset.columns), el tipo de datos que maneja (dataset.info()), la aparicion de valores nulos o la presencia de inconsistencia en algun dato.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
dataset.columns
dataset.shape
dataset.head(1)
dataset.info()
dataset.describe()
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

\end{itemize}

\bigskip

\begin{itemize}

\item[1.3]  {\bf Anonimizacion de los datos y analisis de correlacion}

Para poder analizar los datos, se ha creado un nuevo conjunto de datos anonimizado, en el que se ha eliminado la columna Target del dataset. Ademas de esto, se realiza un analisis de la correlacion entre todas las columnas/variables.

Es importante realizar este paso para detectar altas correlaciones, las cuales suelen indicar multicolinealidad, algo que afecta a los modelos predictivos.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
dataset_anonymized = dataset.drop(["Target"], axis=1)
dataset_anonymized.to_csv('dataset_anonymized.csv', index=False)
dataset_anonymized.corr()
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

El analisis de una correlacion esta identificada con las demas, donde un coeficiente de correlacion cercano a 1 o -1 indica una correlacion fuerte, mientras que un valor cercano a 0 indica una correlacion practicamente inexistente.

\end{itemize}

\newpage

\begin{itemize}

\item[1.4]  {\bf Visualizacion de la matriz de correlacion}

La matriz de correlacion se visualiza mediante un mapa de calor utilizando la libreria seaborn, la cual facilita la identificacion de relaciones positivas o negativas entre las variables.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
fig, ax = plt.subplots(figsize=(9,9))
sb.heatmap(dataset.corr(), linewidth=0.5, annot=True)
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

El grafico resultante proporciona una vision mas clara de las relaciones mas fuertes entre las variables, donde los colores oscuros indican una correlacion alta, mientras que los colores mas claros indican una correlacion baja.

\begin{figure}[h]
  \centering
  \includegraphics[width=14cm, height=14cm]{correlation_heatmap.png}
\end{figure}

\end{itemize}

\newpage

\begin{itemize}

\item[1.5]  {\bf Visualizacion de distribuciones}

Para observar la distribucion de cada una de las variables en el dataset anonimizado, se generan histogramas que permiten identificar patrones y verificar si las variables estan distribuidas de manera normal, poseen sesgos o son valores atipicos.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
columns = dataset_anonymized.columns
fig = plt.figure(figsize=(12,12))
for i in range(0,11):
  ax = plt.subplot(4,4,i+1)
  ax.hist(dataset_anonymized[columns[i]], bins=20, color='blue', edgecolor='black')
  ax.set_title(dataset_anonymized.head(0)[columns[i]].name)
plt.tight_layout()
plt.show()
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Estos histogramas muestran la frecuencia de los valores dentro de ciertos intervalos, algo que ayuda a entender mejor la distribucion de los datos.

\begin{figure}[h]
  \centering
  \includegraphics[width=14cm, height=14cm]{characteristics_frequency.png}
\end{figure}

\end{itemize}

\newpage

\begin{itemize}

\item[1.6]  {\bf Separacion de caracteristicas y etiquetas}

Una vez hecho esto, se separan las caracteristicas (X) y las etiquetas (y) del dataset, lo cual permite preparar los datos para los modelos de machine learning.

En este caso, X contiene la informacion de todas las columnas, exceptuando la columna Target, mientras que y contiene los valores de la variable objetivo Target.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
X = dataset_anonymized
y = dataset.get("Target")
print('Class labels:', np.unique(y))

# RESULTADO
Class labels: [3 4 5 6 7 8]
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

\end{itemize}

\bigskip

\begin{itemize}

\item[1.7]  {\bf Division del dataset en entrenamiento y prueba}

El dataset se divide en dos conjuntos: Uno de entrenamiento (0.75) y otro de prueba (0.25). Esta division se hace de tal forma que la proporcion de clases en ambos conjuntos sea similar, evitando asi posibles desbalanceos.

Es necesario realizar este paso para poder entrenar los modelos con una parte del dataset y luego probar su rendimiento con datos que no han sido vistos previamente.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.25,
 random_state=1, stratify=y)
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

\end{itemize}

\bigskip

\begin{itemize}

\item[1.8]  {\bf Estandarizacion de los datos}

La estandarizacion de los datos asegura que todas las caracteristicas se encuentren en la misma escala, algo que es considerado de gran importancia en modelos de regresion logistica, SVM o redes neuronales, los cuales se veran mas adelante.

Para este caso, se utiliza StandardScaler() para normalizar los datos, los cuales se ajustan a las caracteristicas de los mismos para que tengan media 0 y desviacion estandar 1, lo que se traduce en una mejora de rendimiento a la hora de utilizar los algoritmos mencionados anteriormente.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

\end{itemize}

\bigskip

\begin{itemize}

\item[1.9]  {\bf Verificacion del balance de clases}

Y por ultimo, se evalua el balance de las clases tanto en los datos de entrenamiento como en los datos de prueba para verificar si existe algun tipo de desbalance en las etiquetas, ya que de darse este caso, se podria producir un desbalance significativo que puede afectar a la capacidad predictiva del modelo.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
print('Labels counts in y:', np.bincount(y))
print('Labels counts in y_train:', np.bincount(y_train))
print('Labels counts in y_test:', np.bincount(y_test))

# RESULTADO
Labels counts in y:       [  0   0   0  10  53  681   638   199   18  ]
Labels counts in y_train: [  0   0   0   8  40  511   478   149   13  ]
Labels counts in y_test:  [  0   0   0   2  13  170   160    50    5  ]
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Es importante mencionar que el balance de clases da una idea de si existe predominancia de una clase sobre otra, algo que podria requerir tecnicas de remuestreo para mitigar el sesgo.

\end{itemize}

% ----- 2. CLASIFICACION USANDO TODAS LAS CARACTERISTICAS DE LOS DATOS ----- %

\newpage

\section[2]{Clasificaci\'on usando todas las caracter\'{\i}sticas de los datos}

\begin{itemize}

\item[2.1]  {\bf Introduccion}

En este apartado se presentan los resultados de la clasificacion del conjunto de datos del fichero dataset.csv utilizando 3 metodos de aprendizaje supervisado: Regresion Logistica (Logistic Regression), Maquinas de Soporte Vectorial (SVM) y Arboles de Decision (Random Trees).

Cada uno de estos clasificadores ha sido entrenado usando todas las caracteristicas del dataset. A continuacion, se iran describiendo los parametros utilizados para cada modelo, presentando los resultados obtenidos y discutiendo el rendimiento de cada clasificador.

\end{itemize}

\bigskip

\begin{itemize}

\item[2.2]  {\bf Carga y preparacion de los datos}

Antes de entrenar los modelos, primero se debe cargar el dataset, la anonimizacion de las caracteristicas y la separacion de las caracteristicas (X) respecto a la etiqueta objetivo (y). 

Ademas de esto, tambien se realiza una estandarizacion de las caracteristicas, algo que es muy importante realizar para modelos como la Regresion Logistica (Logistic Regression) y SVM (Support Vector Machines), que son sensibles a las escalas de las variables.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
dataset = pd.read_csv("dataset.csv")
dataset_all_characteristics = dataset.drop(["Target"], axis=1)
X = dataset_all_characteristics
y = dataset.get("Target")

X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.25, 
random_state=1, stratify=y)
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Por otro lado, el dataset se divide en dos conjuntos: Uno de entrenamiento (0.75) y otro de prueba (0.25). Esta division se hace de tal forma que la proporcion de clases en ambos conjuntos sea similar, evitando asi posibles desbalanceos.

\end{itemize}

\bigskip

\begin{itemize}

\item[2.3]  {\bf Regresion Logistica (Logistic Regression)}

La Regresion Logistica es un modelo lineal, por lo que es mas adecuado cuando las caracteristicas tienen relaciones lineales con las etiquetas. En este caso, el modelo muestra un buen rendimiento general, aunque la precision podria mejorar utilizando tecnicas de seleccion de caracteristicas o ajustando mas los parametros.

A continuacion, se explican brevemente los parametros utilizados:

\begin{itemize}

\item
C = 100.0: El parametro de regularizacion C controla la penalizacion aplicada a los errores. Un valor alto de C indica que se permite una menor penalizacion, por lo que el modelo intenta ajustar mas los datos.

\item
solver = 'lbfgs': El solver 'lbfgs' es un optimizador recomendado para solucionar problemas pequeños y medianos.

\item
multi-class = 'ovr': Procedente de las siglas OnevsRest (OVR), significa que para la
clasificacion multiclase, el modelo entrenara un clasificador independiente para cada 
clase.

\end{itemize}

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
lr = LogisticRegression(C=100.0, solver='lbfgs', multi_class='ovr')
lr.fit(X_train_std, y_train)
y_pred = lr.predict(X_test_std)
print('Misclassification samples: %d' % (y_test != y_pred).sum())
print('Accuracy: %.3f' % lr.score(X_test_std, y_test))

# RESULTADO
Misclassification samples: 165
Accuracy. 0.588
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Se han obtenido como resultado 165 muestras mal clasificadas, un numero relativamente bajo para este modelo, y una precision de 0.588, por lo que esa proporcion de  predicciones del modelo es correcta.

\end{itemize}

\bigskip

\begin{itemize}

\item[2.4]  {\bf Maquinas de Soporte Vectorial (SVM)}

SVM con kernel RBF es potente en la captura de relaciones no lineales entre las características. El modelo muestra un rendimiento sólido y, aunque los resultados dependen mucho de la selección de los parámetros gamma y C, el ajuste da buenos resultados.

A continuacion, se explican brevemente los parametros utilizados:

\begin{itemize}

\item
kernel = 'rbf': Se utiliza el kernel radial base (RBF), que es adecuado para problemas no lineales.

\item
gamma = 0.7: Controla el grado de influencia de los puntos individuales. Un valor bajo significa que el área de influencia de cada punto es alta, mientras que un valor alto restringe el área.

\item
C = 30.0: Controla el grado de penalización aplicado a los errores de clasificación. Un valor más alto de C tiende a reducir los errores de clasificación en el conjunto de entrenamiento.

\end{itemize}

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
svm = SVC(kernel='rbf', random_state=1, gamma=0.7, C=30.0)
svm.fit(X_train_std, y_train)
y_pred = svm.predict(X_test_std)
print('Misclassification samples: %d' % (y_test != y_pred).sum())
print('Accuracy: %.3f' % svm.score(X_test_std, y_test))

# RESULTADO
Misclassification samples: 157
Accuracy. 0.608
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Se han obtenido como resultado 157 muestras mal clasificadas, un numero relativamente bajo para este modelo, y una precision de 0.608, una proporcion ligeramente superior a la obtenida con la Regresion Logistica.

\end{itemize}

\bigskip

\begin{itemize}

\item[2.5]  {\bf Arboles de Decision (Random Trees)}

Los árboles de decisión tienden a sobreajustar los datos si no se limitan adecuadamente. En este caso, al limitar la profundidad a 4, se puede evitar el sobreajuste y el modelo puede ser generalizado adecuadamente. Sin embargo, en comparación con los otros modelos, el rendimiento es ligeramente inferior, posiblemente porque el modelo no pudo capturar todas las complejidades de los datos con solo 4 niveles de profundidad.

A continuacion, se explican brevemente los parametros utilizados:

\begin{itemize}

\item
criterion = 'gini': Utiliza el índice de Gini para medir la pureza de los nodos.

\item
max-depth = 4: La profundidad máxima del árbol se fija en 4 para evitar el sobreajuste.

\end{itemize}

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
tree_model = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)
tree_model.fit(X_train, y_train)
y_pred = tree_model.predict(X_test)
print('Misclassification samples: %d' % (y_test != y_pred).sum())
print('Accuracy: %.3f' % tree_model.score(X_test, y_test))

# Visualización del árbol de decisión
tree.plot_tree(tree_model, feature_names=['Col1', 'Col2', 'Col3', 'Col4', 'Col5', 'Col6', 'Col7', 
'Col8', 'Col9', 'Col10', 'Col11'], filled=True)
plt.show()

# RESULTADO
Misclassification samples: 172
Accuracy. 0.570
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Se han obtenido como resultado 172 muestras mal clasificadas, un numero similar a los otros dos modelos, y una precision de 0.570, por lo que esa proporcion de predicciones del modelo es correcta.

\end{itemize}

% ----- 3. CLASIFICACION USANDO 4 CARACTERISTICAS DE LOS DATOS ----- %

\newpage

\section[3]{Clasificaci\'on usando 4 caracter\'{\i}sticas de los datos}

\begin{itemize}

\item[3.1]  {\bf Introduccion}

En este apartado se presentan los resultados de la clasificacion de 4 de las columnas del fichero dataset.csv utilizando 3 metodos de aprendizaje supervisado: Regresion Logistica (Logistic Regression), Maquinas de Soporte Vectorial (SVM) y Arboles de Decision (Random Trees).

El objetivo de esto es comparar el rendimiento de estos modelos al reducir el numero de variables y analizar el impacto de esta reduccion en la precision de los clasificadores.

\end{itemize}

\bigskip

\begin{itemize}

\item[3.2]  {\bf Seleccion de las 4 caracteristicas}

En primer lugar, se realiza un análisis de correlación entre las variables del dataset completo para identificar las características más relevantes. Para ello, se han seleccionado las caracteristicas Col1, Col5, Co7 y Col11.

Las variables seleccionadas son aquellas que mejor coeficiente de correlacion poseen, ademas de poder aportar mucha relevancia para su clasificación, a fin de evitar multicolinealidad y maximizar la capacidad predictiva del modelo.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
dataset_4_characteristics = dataset_anonymized.drop(["Col2", "Col3", "Col4", "Col6",
"Col8", "Col9", "Col10"], axis=1)
dataset_4_characteristics.to_csv('dataset_4_characteristics.csv', index=False)
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

\end{itemize}

\bigskip

\begin{itemize}

\item[3.3]  {\bf Preparacion de los datos}

Una vez hecho esto, el conjunto de datos se ha dividido en 2 subconjuntos: Entrenamiento (0.75) y prueba (0.25), lo que asegura que la distribucion de las clases sea equilibrada.

Ademas, los datos han sido estandarizados mediante StandardScaler(), algo importante en clasificadores como la Regresion Logistica (Logistic Regression) y SVM (Support Vector Machines), que son sensibles a la escala de los datos.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.25,
random_state=1, stratify=y)
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

\end{itemize}

\bigskip

\begin{itemize}

\item[3.4]  {\bf Regresion Logistica (Logistic Regression)}

La Regresion Logistica muestra un rendimiento satisfactorio utilizando solo 4 caracteristicas, lo que sugiere que las variables seleccionadas tienen una relacion significativa con la etiqueta objetivo. A pesar, tambien se puede suponer que la precision obtenida se ha visto limitada por la naturaleza lineal del modelo.

A continuacion, se explican brevemente los parametros utilizados:

\begin{itemize}

\item
C = 100.0: El parametro de regularizacion C controla la penalizacion aplicada a los errores. Un valor alto de C indica que se permite una menor penalizacion, por lo que el modelo intenta ajustar mas los datos.

\item
solver = 'lbfgs': El solver 'lbfgs' es un optimizador recomendado para solucionar problemas pequeños y medianos.

\item
multi-class = 'ovr': Procedente de las siglas OnevsRest (OVR), significa que para la
clasificacion multiclase, el modelo entrenara un clasificador idenpendiente para cada 
clase.

\end{itemize}

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
lr = LogisticRegression(C=100.0, solver='lbfgs', multi_class='ovr')
lr.fit(X_train_std, y_train)
y_pred = lr.predict(X_test_std)
print('Misclassification samples: %d' % (y_test != y_pred).sum())
print('Accuracy: %.3f' % lr.score(X_test_std, y_test))

# RESULTADO
Misclassification samples: 164
Accuracy. 0.590
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Se han obtenido como resultado 164 muestras mal clasificadas y una precision de 0.590.

\end{itemize}

\bigskip

\begin{itemize}

\item[3.5]  {\bf Maquinas de Soporte Vectorial (SVM)}

El uso de un kernel RBF permite capturar relaciones no lineales entre las caracteristicas, por lo que el modelo SVM sigue siendo altamente efectivo incluso teniendo unicamente 4 caracteristicas.

Sin embargo, al reducir el numero de caracteristicas, el modelo podria quedar limitado a la hora de identificar patrones mas complejos.

A continuacion, se explican brevemente los parametros utilizados:

\begin{itemize}

\item
kernel = 'rbf': Se utiliza el kernel radial base (RBF), que es adecuado para problemas no lineales.

\item
gamma = 0.7: Controla el grado de influencia de los puntos individuales. Un valor bajo significa que el área de influencia de cada punto es alta, mientras que un valor alto restringe el área.

\item
C = 30.0: Controla el grado de penalización aplicado a los errores de clasificación. Un valor más alto de C tiende a reducir los errores de clasificación en el conjunto de entrenamiento.

\end{itemize}

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
svm = SVC(kernel='rbf', random_state=1, gamma=0.7, C=30.0)
svm.fit(X_train_std, y_train)
y_pred = svm.predict(X_test_std)
print('Misclassification samples: %d' % (y_test != y_pred).sum())
print('Accuracy: %.3f' % svm.score(X_test_std, y_test))

# RESULTADO
Misclassification samples: 174
Accuracy. 0.565
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Se han obtenido como resultado 174 muestras mal clasificadas y una precision de 0.565.

\end{itemize}

\bigskip

\begin{itemize}

\item[3.6]  {\bf Arboles de Decision (Random Trees)}

El arbol de decision tiene un rendimiento adecuado, aunque presenta una ligera tendencia al sobreajuste al tener solo 4 caracteristicas, algo que se ha solucionado limitando la profundidad, quedando a expensas de la precision en algunas ocasiones.

A continuacion, se explican brevemente los parametros utilizados:

\begin{itemize}

\item
criterion = 'gini': Utiliza el índice de Gini para medir la pureza de los nodos.

\item
max-depth = 4: La profundidad máxima del árbol se fija en 4 para evitar el sobreajuste.

\end{itemize}

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
tree_model = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)
tree_model.fit(X_train, y_train)
tree.plot_tree(tree_model, feature_names=['Col1', 'Col5', 'Col7', 'Col11'], filled=True)
plt.show()
y_pred = tree_model.predict(X_test)
print('Misclassification samples: %d' % (y_test != y_pred).sum())
print('Accuracy: %.3f' % tree_model.score(X_test, y_test))

# RESULTADO
Misclassification samples: 174
Accuracy. 0.565
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Se han obtenido como resultado 174 muestras mal clasificadas y una precision de 0.565.

\end{itemize}

% ----- 4. CLASIFICACION USANDO 4 CARACTERISTICAS DE LOS DATOS ----- %

\newpage

\section[4]{Clasificaci\'on usando 2 caracter\'{\i}sticas de los datos}

\begin{itemize}

\item[4.1]  {\bf Introduccion}

En este apartado se presentan los resultados de la clasificacion de 2 de las columnas del fichero dataset.csv utilizando 3 metodos de aprendizaje supervisado: Regresion Logistica (Logistic Regression), Maquinas de Soporte Vectorial (SVM) y Arboles de Decision (Random Trees).

El objetivo de esto es comparar el rendimiento de estos modelos al reducir el numero de variables y analizar el impacto de esta reduccion en la precision de los clasificadores.

\end{itemize}

\bigskip

\begin{itemize}

\item[4.2]  {\bf Seleccion de las 2 caracteristicas}

En primer lugar, se realiza un análisis de correlación entre las variables del dataset completo para identificar las características más relevantes. Para ello, se han seleccionado las caracteristicas Col1 y Col7.

Las variables seleccionadas son aquellas que mejor coeficiente de correlacion poseen, ademas de poder aportar mucha relevancia para su clasificación, a fin de evitar multicolinealidad y maximizar la capacidad predictiva del modelo.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
dataset_2_characteristics = dataset_anonymized.drop(["Col2", "Col3", "Col4", "Col5",
"Col6", "Col8", "Col9", "Col10", "Col11"], axis=1)
dataset_2_characteristics.to_csv('dataset_2_characteristics.csv', index=False)
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

\end{itemize}

\bigskip

\begin{itemize}

\item[4.3]  {\bf Preparacion de los datos}

Una vez hecho esto, el conjunto de datos se ha dividido en 2 subconjuntos: Entrenamiento (0.75) y prueba (0.25), lo que asegura que la distribucion de las clases sea equilibrada.

Ademas, los datos han sido estandarizados mediante StandardScaler(), algo importante en clasificadores como la Regresion Logistica (Logistic Regression) y SVM (Support Vector Machines), que son sensibles a la escala de los datos.

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.25,
random_state=1, stratify=y)
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

\end{itemize}

\newpage

\begin{itemize}

\item[4.4]  {\bf Regresion Logistica (Logistic Regression)}

La Regresion Logistica muestra un rendimiento aceptable a pesar de la reduccion a solo 2 caracteristicas, lo que indica que las variables seleccionadas aportan informacion relevante para la clasificacion.

Sin embargo, la reduccion a solo 2 variables limita la capacidad del modelo para capturar relaciones mas complejas.

A continuacion, se explican brevemente los parametros utilizados:

\begin{itemize}

\item
C = 100.0: El parametro de regularizacion C controla la penalizacion aplicada a los errores. Un valor alto de C indica que se permite una menor penalizacion, por lo que el modelo intenta ajustar mas los datos.

\item
solver = 'lbfgs': El solver 'lbfgs' es un optimizador recomendado para solucionar problemas pequeños y medianos.

\item
multi-class = 'ovr': Procedente de las siglas OnevsRest (OVR), significa que para la
clasificacion multiclase, el modelo entrenara un clasificador idenpendiente para cada 
clase.

\end{itemize}

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
lr = LogisticRegression(C=100.0, solver='lbfgs', multi_class='ovr')
lr.fit(X_train_std, y_train)
y_pred = lr.predict(X_test_std)
print('Misclassification samples: %d' % (y_test != y_pred).sum())
print('Accuracy: %.3f' % lr.score(X_test_std, y_test))

# RESULTADO
Misclassification samples: 167
Accuracy. 0.583
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Se han obtenido como resultado 167 muestras mal clasificadas y una precision de 0.583.

\end{itemize}

\bigskip

\begin{itemize}

\item[4.5]  {\bf Maquinas de Soporte Vectorial (SVM)}

El SVM con kernel RBF demuestra un rendimiento superior al capturar relaciones no lineales entre las dos caracteristicas seleccionadas.

Sin embargo, en este caso, el clasificador se ha beneficiado de uso de un kernel no lineal, lo que sugiere que aunque el numero de caracteristicas sea bajo, el modelo puede seguir identificando patrones complejos en los datos.

A continuacion, se explican brevemente los parametros utilizados:

\begin{itemize}

\item
kernel = 'rbf': Se utiliza el kernel radial base (RBF), que es adecuado para problemas no lineales.

\item
gamma = 0.7: Controla el grado de influencia de los puntos individuales. Un valor bajo significa que el área de influencia de cada punto es alta, mientras que un valor alto restringe el área.

\item
C = 30.0: Controla el grado de penalización aplicado a los errores de clasificación. Un valor más alto de C tiende a reducir los errores de clasificación en el conjunto de entrenamiento.

\end{itemize}

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
svm = SVC(kernel='rbf', random_state=1, gamma=0.7, C=30.0)
svm.fit(X_train_std, y_train)
y_pred = svm.predict(X_test_std)
print('Misclassification samples: %d' % (y_test != y_pred).sum())
print('Accuracy: %.3f' % svm.score(X_test_std, y_test))

# RESULTADO
Misclassification samples: 167
Accuracy. 0.583
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Se han obtenido como resultado 167 muestras mal clasificadas y una precision de 0.583.

\end{itemize}

\bigskip

\begin{itemize}

\item[4.6]  {\bf Arboles de Decision (Random Trees)}

El Arbol de Decision logra un rendimiento decente, aunque la falta de complejidad en el modelo y al estar trabajando con solo 2 caracteristicas, es algo que podria haber limitado su capacidad predictiva.

Por otro lado, el uso de una profundidad limitada evita que el modelo se sobreajuste a los datos de entrenamiento, algo que se considera muy importante cuando se esta trabajando con un numero tan reducido de caracteristicas.

A continuacion, se explican brevemente los parametros utilizados:

\begin{itemize}

\item
criterion = 'gini': Utiliza el índice de Gini para medir la pureza de los nodos.

\item
max-depth = 4: La profundidad máxima del árbol se fija en 4 para evitar el sobreajuste.

\end{itemize}

\begin{tcolorbox}[width=14cm]
\begin{scriptsize}
\begin{verbatim}
tree_model = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)
tree_model.fit(X_train, y_train)
tree.plot_tree(tree_model, feature_names=['Col1', 'Col7'], filled=True)
plt.show()
y_pred = tree_model.predict(X_test)
print('Misclassification samples: %d' % (y_test != y_pred).sum())
print('Accuracy: %.3f' % tree_model.score(X_test, y_test))

# RESULTADO
Misclassification samples: 178
Accuracy. 0.555
\end{verbatim}
\end{scriptsize}
\end{tcolorbox}

Se han obtenido como resultado 178 muestras mal clasificadas y una precision de 0.555.

\end{itemize}

% ----- 5. COMPARACION DE LOS RESULTADOS Y CONCLUSIONES ----- %

\newpage

\section[5]{Comparaci\'on de los resultados y conclusiones}

\begin{itemize}

\item[5.1]  {\bf Comparacion de los resultados}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{| c | c | c | c | }
      \hline
      Caracteristicas & Regresion Logistica & SVM & Arboles de Decision \\ \hline
      Todas & 0.588 & 0.608 & 0.570 \\
      4     & 0.590 & 0.565 & 0.565 \\
      2     & 0.583 & 0.583 & 0.555 \\ \hline
    \end{tabular}
  \end{center}
\end{table}

El SVM con kernel RBF ha sido el modelo más consistente en cuanto a rendimiento en todos los casos. 

Por otro lado, la Regresión Logística ha ido mostrando un rendimiento sólido pero decreciente a medida que se han ido reduciendo las características. 

Y por ultimo, los Árboles de Decisión han sido los más afectados por la reducción de información, lo que sugiere que su capacidad para construir reglas de decisión eficaces depende más de la cantidad de características disponibles.

\end{itemize}

\bigskip

\begin{itemize}

\item[5.2]  {\bf Conclusiones}

El rendimiento de los clasificadores ha ido dependiendo significativamente del número de características utilizadas. A continuacion se muestran algunos de los aspectos mas importantes a tener en cuenta en cada modelo:

\begin{itemize}

\item
SVM (Support Vector Machines): ha sido el más robusto frente a la reducción de características, adaptándose bien incluso en el caso en el que se han tenido solo 2 caracteristicas.

\item
Regresion Logistica (Logistic Regression): Ha ido mostrando un rendimiento decreciente cuando se han ido reduciendo la cantidad de características, destacando su dependencia en información más completa para realizar predicciones precisas.

\item
Árboles de Decisión (Random Trees): Han necesitado más características para crear reglas complejas y precisas. Sin embargo, con menos variables, su capacidad para capturar patrones importantes se ha visto disminuida.

\end{itemize}

En general, reducir el número de características impacta el rendimiento de los modelos, pero algunos, como el SVM (Support Vector Machines), pueden manejar esta reducción mejor que otros.

\end{itemize}

\end{document}
