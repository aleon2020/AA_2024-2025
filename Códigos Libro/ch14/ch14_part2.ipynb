{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch and Scikit-Learn  \n",
    "# -- Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package version checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add folder to path in order to load from the check_packages.py script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * import sys\n",
    "# Import the sys module, which is a Python standard library module.\n",
    "# This module provides access to variables and functions that interact strongly with the\n",
    "# Python interpreter, such as manipulating module search path and input/output\n",
    "# standard, among others.\n",
    "# * sys.path\n",
    "# It is a list containing the paths in which the Python interpreter looks for modules when\n",
    "# you use import. When you try to import a module, Python searches the paths specified in this\n",
    "# list.\n",
    "# * sys.path.insert(0, '..')\n",
    "# Insert the path '..' (representing the parent directory) at the beginning of the sys.path list.\n",
    "# Adding it in position 0 ensures that when Python looks for modules to import,\n",
    "# first check in the parent directory before continuing with the default paths.\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check recommended package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * from python_environment_check import check_packages\n",
    "# Import the check_packages function from the python_environment_check module. \n",
    "# This module, from its name, appears to be designed to verify that the Python environment \n",
    "# have the correct versions of certain packages installed.\n",
    "# * d = {...}\n",
    "# Defines a dictionary d that contains the names of several packages as keys \n",
    "# (e.g. numpy, scipy, matplotlib, etc.) and as values ​​the minimum versions \n",
    "# required from those packages.\n",
    "# * check_packages(d)\n",
    "# The check_packages function takes as input the dictionary d and probably performs a \n",
    "# check on current Python environment to ensure installed versions \n",
    "# of these packages are at least those specified in the dictionary. If any of the packages \n",
    "# is not installed or has the wrong version, the function may throw an error or \n",
    "# suggest installing/updating the packages.\n",
    "\n",
    "from python_environment_check import check_packages\n",
    "d = {\n",
    "    'numpy': '1.21.2',\n",
    "    'scipy': '1.7.0',\n",
    "    'matplotlib': '3.4.3',\n",
    "    'torch': '1.8.0',\n",
    "    'torchvision': '0.9.0'\n",
    "}\n",
    "check_packages(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Classifying Images with Deep Convolutional Neural Networks (Part 2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "- [Smile classification from face images using a CNN](#Constructing-a-CNN-in-PyTorch)\n",
    "  - [Loading the CelebA dataset](#Loading-the-CelebA-dataset)\n",
    "  - [Image transformation and data augmentation](#Image-transformation-and-data-augmentation)\n",
    "  - [Training a CNN smile classifier](#Training-a-CNN-smile-classifier)\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# * from IPython.display\n",
    "# Import from the display submodule of the IPython package. This module is designed to display \n",
    "# and render different types of data within interactive environments, such as Jupyter Notebooks.\n",
    "# * import Image\n",
    "# Import the Image class from the display module. The Image class is used to display \n",
    "# images in the interactive environment (for example, in a Jupyter Notebook cell).\n",
    "# * %matplotlib inline\n",
    "# This is a magic command specific to IPython/Jupyter Notebook.\n",
    "# Enables display of matplotlib plots directly within cells of the \n",
    "#notebook. Graphics are rendered \"inline\" (within the same notebook) without the need \n",
    "# to open pop-up windows.\n",
    "\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smile classification from face images using CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CelebA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try setting `download=True` in the code cell below, however due to the daily download limits of the CelebA dataset, this will probably result in an error. Alternatively, we recommend trying the following:\n",
    "\n",
    "- You can download the files from the official CelebA website manually (https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) \n",
    "- or use our download link, https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=sharing (recommended). \n",
    "\n",
    "If you use our download link, it will download a `celeba.zip` file, \n",
    "\n",
    "1. which you need to unpack in the current directory where you are running the code. \n",
    "2. In addition, **please also make sure you unzip the `img_align_celeba.zip` file, which is inside the `celeba` folder.**\n",
    "3. Also, after downloading and unzipping the celeba folder, you need to run with the setting `download=False` instead of `download=True` (as shown in the code cell below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For simplicity**, you can also use my link here where I already prepared the directory structure: https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=share_link\n",
    "\n",
    "Download that zip file and place it in the `celeba` folder. Then unzip `img_align_celeba.zip`. And it should work:\n",
    "    \n",
    "![](figures/celeba.webp)\n",
    "    \n",
    "In case you are encountering problems with this approach, please do not hesitate to open a new issue or start a discussion at https://github.com/rasbt/machine-learning-book so that we can provide you with additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "\n",
    "image_path = './'\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='train', \n",
    "    target_type='attr', download=True\n",
    ")\n",
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='valid', \n",
    "    target_type='attr', download=True\n",
    ")\n",
    "celeba_test_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='test', \n",
    "    target_type='attr', download=True\n",
    ")\n",
    "print('Train set:', len(celeba_train_dataset))\n",
    "print('Validation set:', len(celeba_valid_dataset))\n",
    "print('Test set:', len(celeba_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image transformation and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "fig = plt.figure(figsize=(16, 8.5))\n",
    "## Column 1: cropping to a bounding-box\n",
    "ax = fig.add_subplot(2, 5, 1)\n",
    "img, attr = celeba_train_dataset[0]\n",
    "ax.set_title('Crop to a \\nbounding-box', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 6)\n",
    "img_cropped = transforms.functional.crop(img, 50, 20, 128, 128)\n",
    "ax.imshow(img_cropped)\n",
    "\n",
    "## Column 2: flipping (horizontally)\n",
    "ax = fig.add_subplot(2, 5, 2)\n",
    "img, attr = celeba_train_dataset[1]\n",
    "ax.set_title('Flip (horizontal)', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 7)\n",
    "img_flipped = transforms.functional.hflip(img)\n",
    "ax.imshow(img_flipped)\n",
    "\n",
    "## Column 3: adjust contrast\n",
    "ax = fig.add_subplot(2, 5, 3)\n",
    "img, attr = celeba_train_dataset[2]\n",
    "ax.set_title('Adjust constrast', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 8)\n",
    "img_adj_contrast = transforms.functional.adjust_contrast(\n",
    "    img, contrast_factor=2\n",
    ")\n",
    "ax.imshow(img_adj_contrast)\n",
    "\n",
    "## Column 4: adjust brightness\n",
    "ax = fig.add_subplot(2, 5, 4)\n",
    "img, attr = celeba_train_dataset[3]\n",
    "ax.set_title('Adjust brightness', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 9)\n",
    "img_adj_brightness = transforms.functional.adjust_brightness(\n",
    "    img, brightness_factor=1.3\n",
    ")\n",
    "ax.imshow(img_adj_brightness)\n",
    "\n",
    "## Column 5: cropping from image center \n",
    "ax = fig.add_subplot(2, 5, 5)\n",
    "img, attr = celeba_train_dataset[4]\n",
    "ax.set_title('Center crop\\nand resize', size=15)\n",
    "ax.imshow(img)\n",
    "ax = fig.add_subplot(2, 5, 10)\n",
    "img_center_crop = transforms.functional.center_crop(\n",
    "    img, [0.7*218, 0.7*178]\n",
    ")\n",
    "img_resized = transforms.functional.resize(\n",
    "    img_center_crop, size=(218, 178)\n",
    ")\n",
    "ax.imshow(img_resized)\n",
    "# plt.savefig('figures/14_14.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "for i, (img, attr) in enumerate(celeba_train_dataset):\n",
    "    ax = fig.add_subplot(3, 4, i*4+1)\n",
    "    ax.imshow(img)\n",
    "    if i == 0:\n",
    "        ax.set_title('Orig.', size=15)\n",
    "        \n",
    "    ax = fig.add_subplot(3, 4, i*4+2)\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.RandomCrop([178, 178])\n",
    "    ])\n",
    "    img_cropped = img_transform(img)\n",
    "    ax.imshow(img_cropped)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 1: Random crop', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i*4+3)\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ])\n",
    "    img_flip = img_transform(img_cropped)\n",
    "    ax.imshow(img_flip)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 2: Random flip', size=15)\n",
    "\n",
    "    ax = fig.add_subplot(3, 4, i*4+4)\n",
    "    img_resized = transforms.functional.resize(\n",
    "        img_flip, size=(128, 128)\n",
    "    )\n",
    "    ax.imshow(img_resized)\n",
    "    if i == 0:\n",
    "        ax.set_title('Step 3: Resize', size=15)\n",
    "    if i == 2:\n",
    "        break\n",
    "# plt.savefig('figures/14_15.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_smile = lambda attr: attr[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop([178, 178]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop([178, 178]),\n",
    "    transforms.Resize([64, 64]),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "celeba_train_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='train', \n",
    "    target_type='attr', download=False, \n",
    "    transform=transform_train, target_transform=get_smile\n",
    ")\n",
    "torch.manual_seed(1)\n",
    "data_loader = DataLoader(celeba_train_dataset, batch_size=2)\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "num_epochs = 5\n",
    "for j in range(num_epochs):\n",
    "    img_batch, label_batch = next(iter(data_loader))\n",
    "    img = img_batch[0]\n",
    "    ax = fig.add_subplot(2, 5, j + 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f'Epoch {j}:', size=15)\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "    img = img_batch[1]\n",
    "    ax = fig.add_subplot(2, 5, j + 6)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "# plt.savefig('figures/14_16.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_valid_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='valid', \n",
    "    target_type='attr', download=False, \n",
    "    transform=transform, target_transform=get_smile\n",
    ")\n",
    "celeba_test_dataset = torchvision.datasets.CelebA(\n",
    "    image_path, split='test', \n",
    "    target_type='attr', download=False, \n",
    "    transform=transform, target_transform=get_smile\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "celeba_train_dataset = Subset(celeba_train_dataset, \n",
    "                              torch.arange(16000)) \n",
    "celeba_valid_dataset = Subset(celeba_valid_dataset, \n",
    "                              torch.arange(1000)) \n",
    "print('Train set:', len(celeba_train_dataset))\n",
    "print('Validation set:', len(celeba_valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(celeba_train_dataset, \n",
    "                      batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(celeba_valid_dataset, \n",
    "                      batch_size, shuffle=False)\n",
    "test_dl = DataLoader(celeba_test_dataset, \n",
    "                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a CNN Smile classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Sequential()\n",
    "model.add_module(\n",
    "    'conv1', \n",
    "    nn.Conv2d(\n",
    "        in_channels=3, out_channels=32, \n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "model.add_module('relu1', nn.ReLU())        \n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size=2))  \n",
    "model.add_module('dropout1', nn.Dropout(p=0.5)) \n",
    "\n",
    "model.add_module(\n",
    "    'conv2', \n",
    "    nn.Conv2d(\n",
    "        in_channels=32, out_channels=64, \n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "model.add_module('relu2', nn.ReLU())        \n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size=2))   \n",
    "model.add_module('dropout2', nn.Dropout(p=0.5)) \n",
    "\n",
    "model.add_module(\n",
    "    'conv3', \n",
    "    nn.Conv2d(\n",
    "        in_channels=64, out_channels=128, \n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "model.add_module('relu3', nn.ReLU())        \n",
    "model.add_module('pool3', nn.MaxPool2d(kernel_size=2))   \n",
    "\n",
    "model.add_module(\n",
    "    'conv4', \n",
    "    nn.Conv2d(\n",
    "        in_channels=128, out_channels=256, \n",
    "        kernel_size=3, padding=1\n",
    "    )\n",
    ")\n",
    "model.add_module('relu4', nn.ReLU())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((4, 3, 64, 64))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module('pool4', nn.AvgPool2d(kernel_size=8)) \n",
    "model.add_module('flatten', nn.Flatten()) \n",
    "x = torch.ones((4, 3, 64, 64))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module('fc', nn.Linear(256, 1)) \n",
    "model.add_module('sigmoid', nn.Sigmoid()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((4, 3, 64, 64))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, train_dl, valid_dl):\n",
    "    loss_hist_train = [0] * num_epochs\n",
    "    accuracy_hist_train = [0] * num_epochs\n",
    "    loss_hist_valid = [0] * num_epochs\n",
    "    accuracy_hist_valid = [0] * num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            x_batch = x_batch.to(device) \n",
    "            y_batch = y_batch.to(device) \n",
    "            pred = model(x_batch)[:, 0]\n",
    "            loss = loss_fn(pred, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_hist_train[epoch] += loss.item()*y_batch.size(0)\n",
    "            is_correct = ((pred>=0.5).float() == y_batch).float()\n",
    "            accuracy_hist_train[epoch] += is_correct.sum().cpu()\n",
    "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                x_batch = x_batch.to(device) \n",
    "                y_batch = y_batch.to(device) \n",
    "                pred = model(x_batch)[:, 0]\n",
    "                loss = loss_fn(pred, y_batch.float())\n",
    "                loss_hist_valid[epoch] += \\\n",
    "                    loss.item()*y_batch.size(0) \n",
    "                is_correct = \\\n",
    "                    ((pred>=0.5).float() == y_batch).float()\n",
    "                accuracy_hist_valid[epoch] += is_correct.sum().cpu()\n",
    "        loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1} accuracy: '\n",
    "              f'{accuracy_hist_train[epoch]:.4f} val_accuracy: '\n",
    "              f'{accuracy_hist_valid[epoch]:.4f}')\n",
    "    return loss_hist_train, loss_hist_valid, \\\n",
    "        accuracy_hist_train, accuracy_hist_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 30\n",
    "hist = train(model, num_epochs, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = np.arange(len(hist[0])) + 1\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, hist[0], '-o', label='Train loss')\n",
    "ax.plot(x_arr, hist[1], '--<', label='Validation loss')\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Loss', size=15)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_arr, hist[2], '-o', label='Train acc.')\n",
    "ax.plot(x_arr, hist[3], '--<',\n",
    "        label='Validation acc.')\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Accuracy', size=15)\n",
    "# plt.savefig('figures/14_17.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_dl:\n",
    "        x_batch = x_batch.to(device) \n",
    "        y_batch = y_batch.to(device) \n",
    "        pred = model(x_batch)[:, 0]\n",
    "        is_correct = ((pred>=0.5).float() == y_batch).float()\n",
    "        accuracy_test += is_correct.sum().cpu()\n",
    "accuracy_test /= len(test_dl.dataset)\n",
    "print(f'Test accuracy: {accuracy_test:.4f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_batch)[:, 0] * 100\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "for j in range(10, 20):\n",
    "    ax = fig.add_subplot(2, 5, j-10+1)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(x_batch[j].cpu().permute(1, 2, 0))\n",
    "    if y_batch[j] == 1:\n",
    "        label = 'Smile'\n",
    "    else:\n",
    "        label = 'Not Smile'\n",
    "    ax.text(\n",
    "        0.5, -0.15, \n",
    "        f'GT: {label:s}\\nPr(Smile)={pred[j]:.0f}%', \n",
    "        size=16, \n",
    "        horizontalalignment='center',\n",
    "        verticalalignment='center', \n",
    "        transform=ax.transAxes\n",
    "    )\n",
    "# plt.savefig('figures/figures-14_18.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "path = 'models/celeba-cnn.ph'\n",
    "torch.save(model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Readers may ignore the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a command in the terminal from a Python environment (such as a Jupyter Notebook or a \n",
    "# script that allows system commands to convert a Jupyter notebook to a file Python script. \n",
    "# * !\n",
    "# This symbol is used in environments such as Jupyter Notebooks to execute system commands \n",
    "# operational directly from the notebook. In this case, the command is an execution of a \n",
    "# Python Script.\n",
    "# * python convert_notebook_to_script.py\n",
    "# This command runs a Python script called convert_notebook_to_script.py. This file \n",
    "# is located in the previous directory (../ indicates that it is one level up in the system \n",
    "# files). The purpose of this script is to convert a Jupyter notebook (.ipynb) into a \n",
    "# Python script file (.py).\n",
    "# * --input ch14_part2.ipynb\n",
    "# This is an option or argument that tells the script what the input file is, in this \n",
    "# case, the notebook ch14_part2.ipynb.\n",
    "# * --output ch14_part2.py\n",
    "# This option tells the script to save the output (the converted file) with the name\n",
    "# ch14_part2.py, which is a Python script.\n",
    "\n",
    "! python ../.convert_notebook_to_script.py --input ch14_part2.ipynb --output ch14_part2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
