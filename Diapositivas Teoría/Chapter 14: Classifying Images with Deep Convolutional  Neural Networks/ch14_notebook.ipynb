{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 14: Clasificación de Imágenes con Redes Neuronales Convolucionales Profundas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Componentes básicos de las CNN](#componentes-básicos-de-las-cnn)\n",
    "- [Comprensión de las CNN y las jerarquías de funciones](#comprensión-de-las-cnn-y-las-jerarquías-de-funciones)\n",
    "- [Realización de Convoluciones Discretas](#realización-de-convoluciones-discretas)\n",
    "- [Convoluciones Discretas en 1 Dimensión](#convoluciones-discretas-en-1-dimensión)\n",
    "- [Relleno de entradas para controlar el tamaño de los mapas de características de salida](#relleno-de-entradas-para-controlar-el-tamaño-de-los-mapas-de-características-de-salida)\n",
    "- [Determinación del tamaño de salida de la convolución](#determinación-del-tamaño-de-salida-de-la-convolución)\n",
    "- [Convoluciones Discretas en 2 Dimensiones](#convoluciones-discretas-en-2-dimensiones)\n",
    "- [Submuestreo o agrupación de capas](#submuestreo-o-agrupación-de-capas)\n",
    "- [Trabajo con múltiples canales de entrada o de color](#trabajo-con-múltiples-canales-de-entrada-o-de-color)\n",
    "- [Funciones de activación](#funciones-de-activación)\n",
    "- [Funciones de pérdida para clasificación](#funciones-de-pérdida-para-clasificación)\n",
    "- [Implementación de una CNN profunda usando PyTorch](#implementación-de-una-cnn-profunda-usando-pytorch)\n",
    "- [Convertir Jupyter Notebook a Fichero Python](#convertir-jupyter-notebook-a-fichero-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Componentes básicos de las CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales convolucionales (CNN) son una familia de modelos que fueron originalmente inspirados en cómo funciona la corteza visual del cerebro humano cuando reconocemos objetos.\n",
    "\n",
    "El desarrollo de las CNN se remonta a la década de 1990, cuando Yann LeCun y sus colegas propusieron una arquitectura NN novedosa para clasificar dígitos escritos a mano a partir de imágenes.\n",
    "\n",
    "Debido al excelente desempeño de las CNN para tareas de clasificación de imágenes, este tipo particular de NN feedforward ganó mucha atención y condujo a enormes mejoras en el aprendizaje automático para visión computacional.\n",
    "\n",
    "Varios años después, en 2019, Yann LeCun recibió el premio Turing (el premio más prestigioso en informática) por sus contribuciones al campo de inteligencia artificial (IA), junto con otros dos investigadores, Yoshua Bengio y Geoffrey Hinton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprensión de las CNN y las jerarquías de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La extracción exitosa de características relevantes es clave para el rendimiento de cualquier algoritmo de aprendizaje automático y el aprendizaje automático tradicional. Los modelos se basan en características de entrada que pueden provenir de un experto en el dominio o se basan en la selección o extracción de características computacionales técnicas.\n",
    "\n",
    "Ciertos tipos de NN, como las CNN, pueden aprender automáticamente características de datos sin procesar que son más útiles para una tarea particular. Por esta razón, es común considerar las capas CNN como características para extraer:\n",
    "\n",
    "• Las primeras capas (las que están inmediatamente después de la capa de entrada) extraen características de bajo nivel a partir de datos sin procesar, y las capas posteriores (a menudo capas completamente conectadas, como en un perceptrón multicapa (MLP) utiliza estas características para predecir un objetivo continuo (etiqueta de valor o clase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_01.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_01.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ciertos tipos de NN multicapa y, en particular, las CNN profundas, construyen la llamada jerarquía de características combinando las características de bajo nivel en forma de capas para formar características de alto nivel.\n",
    "\n",
    "Por ejemplo, si estamos tratando con imágenes, entonces el nivel bajo de características, como bordes y manchas, se extraen de capas anteriores, que se combinan para formar entidades de alto nivel.\n",
    "\n",
    "Estas características de alto nivel pueden formar formas más complejas, como los contornos generales de objetos como edificios, gatos o perros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_02.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_02.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una CNN calcula mapas de características a partir de una imagen de entrada, donde cada elemento proviene de un parche local de píxeles en la imagen de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_03.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_03.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las CNN funcionan muy bien en tareas relacionadas con imágenes y eso se debe en gran medida a dos ideas importantes:\n",
    "\n",
    "• Conectividad escasa: Un único elemento en el mapa de características está conectado solo a una pequeña porción de píxeles (esto es muy diferente de conectarse a toda la imagen de entrada, como en el caso de los MLP).\n",
    "\n",
    "• Compartir parámetros: Se utilizan los mismos pesos para diferentes parches de la imagen de entrada.\n",
    "\n",
    "Como consecuencia directa de estas dos ideas, la sustitución de un sistema convencional, MLP completamente conectado con una capa de convolución disminuye sustancialmente la cantidad de pesos (parámetros) en la red, por lo que veremos una mejora en la capacidad de capturar características relevantes.\n",
    "\n",
    "En el contexto de los datos de imágenes, tiene sentido suponer que las personas cercanas a los píxeles suelen ser más relevantes entre sí que los píxeles que están más alejados.\n",
    "\n",
    "Normalmente, las CNN se componen de varias capas de submuestreo convolucionales que van seguidas de una o más capas conectadas al final. Las capas completamente conectadas son esencialmente un MLP.\n",
    "\n",
    "Las capas de submuestreo, comúnmente conocidas como capas de agrupación, no tienen parámetros que se puedan aprender.\n",
    "No hay pesos ni unidades de polarización en las capas de agrupación. Sin embargo, tanto la capa convolucional como la completamente conectada tienen pesos y sesgos que se optimizan durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realización de Convoluciones Discretas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender cómo funcionan las operaciones de convolución, comencemos con un convolución en una dimensión, que a veces se utiliza para trabajar con ciertos tipos de datos de secuencia, como texto.\n",
    "\n",
    "Una convolución discreta (o simplemente convolución) es un elemento fundamental de operación en una CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convoluciones Discretas en 1 Dimensión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una convolución discreta para dos vectores, x y w, se denota por y = x * w, en el cual el vector x es nuestra entrada (a veces llamada señal) y w se llama filtro o núcleo.\n",
    "\n",
    "Una convolución discreta se define matemáticamente de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_04.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_04.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hecho de que la suma pase por índices de –∞ a +∞ parece extraño, principalmente porque en las aplicaciones de aprendizaje automático, siempre tratamos con vectores de características finitas.\n",
    "\n",
    "Por ejemplo, si x tiene 10 características con índices 0, 1, 2, ..., 8, 9, entonces los índices –∞:-1 y 10:+∞ están fuera de los límites para x.\n",
    "\n",
    "Por lo tanto, para calcular correctamente la suma que se muestra en el punto anterior fórmula, se supone que x y w están llenos de ceros.\n",
    "\n",
    "Esto dará como resultado un vector de salida, y, que también tiene tamaño infinito, con muchos ceros también.\n",
    "\n",
    "Dado que esto no es útil en situaciones prácticas, x se rellena sólo con un valor finito número de ceros.\n",
    "\n",
    "Este proceso se llama relleno con ceros o simplemente relleno. Aquí, el número de ceros rellenados en cada lado se indica con p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_05.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_05.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que la entrada original, x, y el filtro, w, tienen n y m elementos, respectivamente, donde m ≤ n.\n",
    "\n",
    "El vector acolchado, xp, tiene tamaño n + 2p. La fórmula práctica para calcular una convolución discreta cambiará a lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_06.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_06.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de que el tamaño del relleno es cero (p=0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_07.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_07.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que el filtro girado, wr, es desplazado en dos celdas cada vez que cambiamos.\n",
    "\n",
    "Este cambio es otro hiperparámetro de una circunvolución, la zancada, s.\n",
    "\n",
    "En este ejemplo, la zancada es dos, s = 2.\n",
    "\n",
    "Tenga en cuenta que la zancada debe ser número positivo menor que el tamaño del vector de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_08.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_08.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relleno de entradas para controlar el tamaño de los mapas de características de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay tres modos de relleno que se utilizan comúnmente en práctica: plena, igual y válida.\n",
    "\n",
    "• En modo completo, el parámetro de relleno, p, se establece en p = m – 1. Relleno completo aumenta las dimensiones de la salida; por lo tanto, rara vez se usa en arquitecturas CNN.\n",
    "\n",
    "• Generalmente se utiliza el mismo modo de relleno para garantizar que el vector de salida tiene el mismo tamaño que el vector de entrada, x. En este caso, el parámetro de relleno, p, se calcula según el tamaño del filtro, junto con el requisito de que el tamaño de entrada y el tamaño de salida sean los mismos.\n",
    "\n",
    "• Finalmente, calcular una convolución en modo válido se refiere al caso donde p = 0 (sin relleno).\n",
    "\n",
    "El modo de relleno más utilizado en las CNN es el mismo relleno.\n",
    "\n",
    "Una de sus ventajas sobre los otros modos de relleno es la misma. El relleno preserva el tamaño del vector, lo que facilita el diseño y una arquitectura de red más conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_09.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_09.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinación del tamaño de salida de la convolución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que el vector de entrada es de tamaño n y el filtro es de talla m.\n",
    "\n",
    "Entonces, el tamaño de la salida resultante de y = x * w, con relleno p y zancada s, se determinarían de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_10.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_10.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_11.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_11.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convoluciones Discretas en 2 Dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando trabajamos con entradas 2D, como una matriz, 𝑿𝑛1×𝑛2, y la matriz de filtro, 𝑾𝑚1×𝑚2, donde 𝑚1 ≤ 𝑛1 y 𝑚2 ≤ 𝑛2, entonces la matriz 𝒀=𝑿*𝑾 es el resultado de una convolución 2D entre 𝑿 y 𝑾. Esto es definido matemáticamente de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_12.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_12.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_13.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_13.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_14.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_14.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submuestreo o agrupación de capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El submuestreo se aplica normalmente en dos formas de agrupación operaciones en CNN: Agrupación máxima y agrupación media (también conocida como agrupación promedio).\n",
    "\n",
    "La capa de agrupación generalmente se denota por 𝑃𝑛1×𝑛2. Aquí el subíndice determina el tamaño del vecindario (el número de píxeles vecinos en cada dimensión) donde la operación máxima o media es realizada. A este tipo de vecindario nos referimos como tamaño de la agrupación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_15.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_15.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La agrupación (max-pooling) introduce una invariancia local. Esto significa que pequeños cambios en EL vecindario local no cambia el resultado de la agrupación máxima.\n",
    "\n",
    "Por lo tanto, ayuda a generar características que sean más resistentes al ruido en la entrada de datos.\n",
    "\n",
    "En el siguiente ejemplo se muestra que la agrupación máxima de dos diferentes matrices de entrada, X1 y X2, dan como resultado la misma salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_16.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_16.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La agrupación disminuye el tamaño de las características, lo que resulta en una mayor capacidad de eficiencia computacional. Además, reducir el número de funciones puede reducir también el grado de sobreajuste.\n",
    "\n",
    "Tradicionalmente, se supone que la agrupación no se superpone.\n",
    "\n",
    "La agrupación generalmente se realiza en vecindarios que no se superponen, lo que se puede hacer estableciendo el parámetro de zancada igual al tamaño de la agrupación. Por ejemplo, una capa de agrupación que no se superpone, 𝑃𝑛1×𝑛2, requiere un paso parámetro s = (n1, n2).\n",
    "\n",
    "Si bien la agrupación sigue siendo una parte esencial de muchas arquitecturas de CNN, también se han desarrollado varias arquitecturas CNN sin utilizar capas de agrupación.\n",
    "\n",
    "En lugar de utilizar capas de agrupación para reducir el tamaño de la entidad, los investigadores usan capas convolucionales con un paso de 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo con múltiples canales de entrada o de color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una entrada a una capa convolucional puede contener una o más matrices 2D o matrices con dimensiones N1×N2 (por ejemplo, la altura de la imagen y ancho en píxeles).\n",
    "\n",
    "Estas matrices N1×N2 se llaman canales.\n",
    "\n",
    "Las implementaciones convencionales de capas convolucionales esperan una representación tensorial de rango 3 como entrada, por ejemplo, una representación tridimensional. matriz, 𝑿𝑁1×𝑁2×𝐶𝑖n, donde Cin es el número de canales de entrada.\n",
    "\n",
    "Por ejemplo, consideremos imágenes como entrada a la primera capa de una CNN. Si la imagen está coloreada y utiliza el modo de color RGB, entonces Cin = 3 (para los canales de color rojo, verde y azul en RGB).\n",
    "\n",
    "Sin embargo, si la imagen está en escala de grises, entonces tenemos Cin=1, porque solo hay un canal con los valores de intensidad de píxeles en escala de grises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_17.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_17.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tienen diferentes funciones de activación, como ReLU, sigmoide y tanh.\n",
    "\n",
    "Algunas de estas funciones de activación, como ReLU, se utilizan principalmente en las capas intermedias (ocultas) de una NN para agregar no linealidades a nuestro modelo.\n",
    "\n",
    "Otros, como sigmoide (para binario) y softmax (para multiclase), se agregan en la última capa (salida), lo que da como resultado probabilidades de membresía de clase como salida del modelo.\n",
    "\n",
    "Si las activaciones sigmoidea o softmax no están incluidas en el capa de salida, entonces el modelo calculará los logits en lugar de las probabilidades de pertenencia a una clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de pérdida para clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centrarse en los problemas de clasificación, dependiendo del tipo de problema (binario versus multiclase) y el tipo de salida (logits versus probabilidades), debemos elegir la función de pérdida apropiada para entrenar nuestro modelo.\n",
    "\n",
    "• La entropía cruzada binaria es la función de pérdida para una clasificación binaria (con una sola unidad de salida).\n",
    "\n",
    "• La entropía cruzada categórica es la función de pérdida para clasificación multiclase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_18.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_18.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Binary Cross-entropy\n",
    "logits = torch.tensor([0.8])\n",
    "probas = torch.sigmoid(logits)\n",
    "target = torch.tensor([1.0])\n",
    "bce_loss_fn = nn.BCELoss()\n",
    "bce_logits_loss_fn = nn.BCEWithLogitsLoss()\n",
    "print(f'BCE (w Probas): {bce_loss_fn(probas, target):.4f}')\n",
    "print(f'BCE (w Logits): {bce_logits_loss_fn(logits, target):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Categorical Cross-entropy\n",
    "logits = torch.tensor([[1.5, 0.8, 2.1]])\n",
    "probas = torch.softmax(logits, dim=1)\n",
    "target = torch.tensor([2])\n",
    "cce_loss_fn = nn.NLLLoss()\n",
    "cce_logits_loss_fn = nn.CrossEntropyLoss()\n",
    "print(f'CCE (w Logits): {cce_logits_loss_fn(logits, target):.4f}')\n",
    "print(f'CCE (w Probas): {cce_loss_fn(torch.log(probas), target):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de una CNN profunda usando PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/14_19.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/14_19.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "image_path = './'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mnist_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=image_path, train=True,\n",
    "    transform=transform, download=True\n",
    ")\n",
    "from torch.utils.data import Subset\n",
    "mnist_valid_dataset = Subset(mnist_dataset, \n",
    "                             torch.arange(10000))\n",
    "mnist_train_dataset = Subset(mnist_dataset, \n",
    "                             torch.arange(\n",
    "                                 10000, len(mnist_dataset)\n",
    "                            ))\n",
    "mnist_test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root=image_path, train=False,\n",
    "    transform=transform, download=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(mnist_train_dataset,\n",
    "                      batch_size,\n",
    "                      shuffle=True)\n",
    "\n",
    "valid_dl = DataLoader(mnist_valid_dataset,\n",
    "                      batch_size,\n",
    "                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module(\n",
    "    'conv1',\n",
    "    nn.Conv2d(\n",
    "        in_channels=1, out_channels=32,\n",
    "        kernel_size=5, padding=2\n",
    "    )\n",
    ")\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('pool1', nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module(\n",
    "    'conv2',\n",
    "    nn.Conv2d(\n",
    "        in_channels=32, out_channels=64,\n",
    "        kernel_size=5, padding=2\n",
    "    )\n",
    ")\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('pool2', nn.MaxPool2d(kernel_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al proporcionar la forma de entrada como una tupla (4, 1, 28, 28) (4 imágenes dentro del lote, 1 canal y tamaño de imagen 28×28), especificado en este ejemplo, calculamos la salida tenga una forma (4, 64, 7, 7), indicando mapas de características con 64 canales y un tamaño espacial de 7×7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((4, 1, 28, 28))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module('flatten', nn.Flatten())\n",
    "x = torch.ones((4, 1, 28, 28))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module('fc1', nn.Linear(3136, 1024))\n",
    "model.add_module('relu3', nn.ReLU())\n",
    "model.add_module('dropout', nn.Dropout(p=0.5))\n",
    "model.add_module('fc2', nn.Linear(1024, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs, train_dl, valid_dl):\n",
    "    loss_hist_train = [0] * num_epochs\n",
    "    accuracy_hist_train = [0] * num_epochs\n",
    "    loss_hist_valid = [0] * num_epochs\n",
    "    accuracy_hist_valid = [0] * num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            x_batch = x_batch.cpu()\n",
    "            y_batch = y_batch.cpu()\n",
    "            pred = model(x_batch)\n",
    "            loss = loss_fn(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_hist_train[epoch] += loss.item()*y_batch.size(0)\n",
    "            is_correct = (\n",
    "                torch.argmax(pred, dim=1) == y_batch\n",
    "            ).float()\n",
    "            accuracy_hist_train[epoch] += is_correct.sum()\n",
    "        loss_hist_train[epoch] /= len(train_dl.dataset)\n",
    "        accuracy_hist_train[epoch] /= len(train_dl.dataset)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in valid_dl:\n",
    "                x_batch = x_batch.cpu()\n",
    "                y_batch = y_batch.cpu()\n",
    "                pred = model(x_batch)\n",
    "                loss = loss_fn(pred, y_batch)\n",
    "                loss_hist_valid[epoch] += \\\n",
    "                    loss.item()*y_batch.size(0)\n",
    "                is_correct = (\n",
    "                    torch.argmax(pred, dim=1) == y_batch\n",
    "                ).float()\n",
    "                accuracy_hist_valid[epoch] += is_correct.sum()\n",
    "        loss_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "        accuracy_hist_valid[epoch] /= len(valid_dl.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1} accuracy: '\n",
    "              f'{accuracy_hist_train[epoch]:.4f} val_accuracy: '\n",
    "              f'{accuracy_hist_valid[epoch]:.4f}')\n",
    "        \n",
    "    return loss_hist_train, loss_hist_valid, \\\n",
    "        accuracy_hist_train, accuracy_hist_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 20\n",
    "hist = train(model, num_epochs, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_arr = np.arange(len(hist[0])) + 1\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x_arr, hist[0], '--o', label='Train loss')\n",
    "ax.plot(x_arr, hist[1], '--c', label='Validation loss')\n",
    "ax.legend(fontsize=15)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x_arr, hist[2], '--o', label='Train acc.')\n",
    "ax.plot(x_arr, hist[3], '--c', \n",
    "        label='Validation acc.')\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.set_ylabel('Accuracy', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(mnist_test_dataset.data.unsqueeze(1) / 255.)\n",
    "is_correct = (\n",
    "    torch.argmax(pred, dim=1) == mnist_test_dataset.targets\n",
    ").float()\n",
    "print(f'Test accuracy: {is_correct.mean():4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir Jupyter Notebook a Fichero Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .convert_notebook_to_script.py --input ch14_notebook.ipynb --output ch14_notebook.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
