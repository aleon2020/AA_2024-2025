{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Lenguaje Grandes (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [IA generativa](#ia-generativa)\n",
    "- [¿Qué es un modelo de lenguaje?](#qué-es-un-modelo-de-lenguaje)\n",
    "- [¿Qué es un LLM?](#qué-es-un-llm)\n",
    "- [¿Qué tamaño tiene un LLM?](#qué-tamaño-tiene-un-llm)\n",
    "- [¿Cómo se interactua con los LLM?](#cómo-se-interactua-con-los-llm)\n",
    "- [Casos de uso de LLM](#casos-de-uso-de-llm)\n",
    "- [¿Cómo funcionan los LLM?](#cómo-funcionan-los-llm)\n",
    "- [Generación de texto con LLM](#generación-de-texto-con-llm)\n",
    "- [Variaciones de la arquitectura LLM](#variaciones-de-la-arquitectura-llm)\n",
    "- [Prompting](#prompting)\n",
    "- [Fine-tuning](#fine-tuning)\n",
    "- [LoRA](#lora)\n",
    "- [Prompting suave](#prompting-suave)\n",
    "- [Parámetros de configuración para inferencia](#parámetros-de-configuración-para-inferencia)\n",
    "- [Recursos](#recursos)\n",
    "- [Convertir Jupyter Notebook a Fichero Python](#convertir-jupyter-notebook-a-fichero-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IA generativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Inteligencia artificial generativa (IA) es un término amplio que se puede utilizar para cualquier sistema de IA cuya función principal sea generar contenido como imágenes, texto, código, etc.\n",
    "\n",
    "Algunos ejemplos de sistemas de IA generativa son:\n",
    "\n",
    "• Generadores de imágenes (como Midjourney o Stable Diffusion).\n",
    "\n",
    "• Generadores de texto (como GPT-4, Claude o Llama).\n",
    "\n",
    "• Generadores de código (como Github Copilot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es un modelo de lenguaje?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de lenguaje es un sistema de inteligencia artificial generativo basado en una máquina/modelo de aprendizaje que pretende generar un lenguaje plausible.\n",
    "\n",
    "Estos modelos funcionan estimando la probabilidad de que un token o secuencia de tokens que ocurren dentro de una secuencia más larga de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_01.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_01.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es un LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los primeros modelos de lenguaje eran muy limitados y, por ejemplo, sólo se podía predecir la probabilidad de una sola ficha. Sin embargo, los modelos de lenguaje modernos pueden predecir la probabilidad de gran secuencia de tokens como oraciones, párrafos o incluso documentos enteros.\n",
    "\n",
    "Los modelos de lenguaje moderno se denominan “modelos de lenguaje grande” (LLM) y se refieren a Modelos de Lenguaje que utilizan técnicas de aprendizaje profundo con un gran número de parámetros (desde millones hasta incluso billones).\n",
    "\n",
    "Estos modelos pueden capturar patrones complejos en el lenguaje y generar texto que a menudo es indistinguible de lo escrito por humanos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué tamaño tiene un LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Grande” en los modelos de lenguaje se refiere al número de parámetros en el modelo (a veces también al número de palabras en el conjunto de datos).\n",
    "\n",
    "Los parámetros son los pesos que el modelo de aprendizaje profundo aprendió durante el proceso de capacitación.\n",
    "\n",
    "Los LLM tienen millones, miles de millones o incluso billones de parámetros. Por ejemplo:\n",
    "\n",
    "• BERT: 110M parámetros.\n",
    "\n",
    "• PaLM 2: parámetros 340B.\n",
    "\n",
    "• GPT-4: parámetros 1T.\n",
    "\n",
    "En general, cuantos más parámetros tenga un LLM, más sofisticadas son las tareas que puede realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_02.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_02.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo se interactua con los LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El texto que se le pasa a un LLM se conoce como mensaje.\n",
    "\n",
    "El espacio que está disponible para el mensaje se llama ventana de contexto, y ésta suele ser lo suficientemente grande para unos pocos miles de palabras pero difiere de un modelo a otro.\n",
    "\n",
    "El resultado del modelo se llama finalización.\n",
    "\n",
    "El acto de utilizar el modelo para generar texto se conoce como inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_03.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_03.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casos de uso de LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La predicción de la siguiente palabra es el concepto básico detrás del texto LLM generado.\n",
    "\n",
    "Esta técnica sencilla se utiliza para una variedad de tareas en LLM:\n",
    "\n",
    "• Redacción de información.\n",
    "\n",
    "• Resumen.\n",
    "\n",
    "• Traducción.\n",
    "\n",
    "• Recuperación de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_04.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_04.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo funcionan los LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de los primeros modelos de lenguaje se crearon usando modelos de redes neuronales recurrentes (RNN): memoria larga a corto plazo (LSTM) o redes de unidades recurrentes cerradas (GRU).\n",
    "\n",
    "Sin embargo, los RNN estaban limitados por la cantidad de computación y memoria necesaria para desempeñarse bien en tareas generativas (RNN intenta recordar toda la secuencia de entrada).\n",
    "\n",
    "Además, los RNN sufren el problema del gradiente de desaparición que hace que sea difícil actualizar los pesos en las capas de la red, comprometiendo la capacidad de aprendizaje y de la red.\n",
    "\n",
    "Un avance clave en los modelos de lenguaje fue la introducción en 2017 de Transformers, una arquitectura diseñada en torno a la idea de atención.\n",
    "\n",
    "Esto hizo posible procesar secuencias más largas centrándose en los aspectos más parte importante de la entrada y la resolución de problemas de memoria encontrados en anteriores modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_05.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_05.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La novedosa arquitectura del transformador ha producido el progreso en IA generativa en general y LLM en particular que vemos hoy.\n",
    "\n",
    "Arquitectura del transformador:\n",
    "\n",
    "• Se puede escalar de manera eficiente para utilizar una GPU de múltiples núcleos.\n",
    "\n",
    "• Puede procesar datos de entrada en paralelo, haciendo uso de los conjuntos de datos de entrenamiento más grandes.\n",
    "\n",
    "• Es capaz de aprender a prestar atención al significado de las palabras que han sido procesados.\n",
    "\n",
    "El poder de la arquitectura del transformador reside en su capacidad para aprender la relevancia y el contexto de todas las palabras de una oración.\n",
    "\n",
    "No sólo a cada palabra al lado de su vecina, sino a cada otra palabra en una oración.\n",
    "\n",
    "Aplicar pesos de atención a esas relaciones para que el modelo aprenda la relevancia de cada palabra entre sí, sin importar dónde se encuentren en el aporte.\n",
    "\n",
    "Los pesos de atención se aprenden durante la formación del LLM.\n",
    "\n",
    "Es útil ilustrar los pesos de atención entre cada palabra y cada dos palabras con un mapa de atención.\n",
    "\n",
    "Este mecanismo para capturar dependencias y relaciones identificando y sopesando la importancia dentro de una secuencia de entrada se llama autoatención.\n",
    "\n",
    "La autoatención es uno de los atributos clave de la arquitectura del transformador.\n",
    "\n",
    "Diagrama simplificado de la arquitectura del transformador original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_06.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_06.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de pasar textos al modelo, las palabras deben tokenizarse. Esto convierte las palabras en números, donde cada número representa una posición en un diccionario (vocabulario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_07.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_07.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que la entrada está representada por números, se puede pasar a la capa de incrustación.\n",
    "\n",
    "Esta capa es un espacio vectorial entrenable de alta dimensión donde cada token se representa como un vector multidimensional y ocupa una ubicación única dentro de ese espacio.\n",
    "\n",
    "La intuición es que los vectores aprenden a codificar el significado y el contexto de tokens individuales en la secuencia de entrada.\n",
    "\n",
    "Es posible ver que las palabras relacionadas están ubicadas cerca unas de otras en el espacio de empotramiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_08.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_08.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La información posicional se agrega explícitamente al modelo para retener la información sobre el orden de las palabras en una oración.\n",
    "\n",
    "La codificación posicional es el esquema a través del cual se mantiene el orden del conocimiento de los objetos en una secuencia.\n",
    "\n",
    "El modelo procesa cada uno de los tokens de entrada en paralelo, por lo que agregando la codificación posicional, la información sobre el orden y la relevancia de la posición de la palabra en la frase no se pierde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_09.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_09.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se suman las incorporaciones de tokens y las incorporaciones posicionales, y los vectores resultantes se pasan a la capa de autoatención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_10.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_10.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la capa de autoatención, el modelo analiza las relaciones entre los tokens en la secuencia de entrada y atiende a diferentes partes de la secuencia de entrada para capturar mejor las dependencias contextuales entre las palabras.\n",
    "\n",
    "Pero esto no ocurre sólo una vez: la arquitectura del transformador en realidad tiene autoatención de múltiples cabezas. Esto significa que se aprenden múltiples conjuntos de pesos o cabezas de autoatención en paralelo independientemente unos de otros. La intuición es que cada jefe de autoatención aprenderá un aspecto diferente de idioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_11.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_11.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_12.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_12.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de que se hayan aplicado los pesos de atención a los datos de entrada, se procesa la salida a través de una red de alimentación directa completamente conectada.\n",
    "\n",
    "La salida de esta capa es un vector de logits proporcional a la puntuación de probabilidad para cada token en el diccionario.\n",
    "\n",
    "En el decodificador, estos logits se pasan a una capa final de softmax, donde se normalizan en una puntuación de probabilidad para cada palabra.\n",
    "\n",
    "Este resultado incluye una probabilidad para cada palabra en el vocabulario.\n",
    "\n",
    "Una sola ficha tendrá una puntuación superior al resto. Este es el token más probable predicho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_13.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_13.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de texto con LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra un ejemplo del proceso de predicción general de un extremo a otro en una arquitectura transformadora. Este ejemplo consiste en una tarea secuencia a secuencia (específicamente una tarea de traducción), que era el objetivo original de los diseñadores de la arquitectura transformadora.\n",
    "\n",
    "Primero, las palabras de entrada se tokenizan y estos tokens se insertan en la entrada en el lado del codificador de la red.\n",
    "\n",
    "Los tokens se pasan a través de la capa de incrustación y luego se introducen en las capas de atención de múltiples cabezas.\n",
    "\n",
    "Las salidas de las capas de atención de múltiples cabezas se alimentan a través de una red de avance a la salida del codificador.\n",
    "\n",
    "En este punto, los datos que salen del codificador son una representación profunda de la estructura y el significado de la secuencia de entrada.\n",
    "\n",
    "Esta representación se inserta en el medio del decodificador para influir en los mecanismos de autoatención del decodificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_14.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_14.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se agrega un token de inicio de secuencia a la entrada del decodificador.\n",
    "\n",
    "Esto hace que el decodificador prediga el siguiente token basándose en la comprensión contextual que proporciona el codificador.\n",
    "\n",
    "La salida de las capas de autoatención del decodificador pasa a través del red de avance del decodificador y a través de una capa de salida final softmax. En este punto, se obtiene la primera ficha.\n",
    "\n",
    "Este bucle continúa, pasando el token de salida nuevamente al decodificador de entrada para desencadenar la generación del siguiente token, hasta que el modelo prediga un token de fin de secuencia.\n",
    "\n",
    "En este punto, la secuencia final de tokens se puede destokenizar en palabras, obteniendo así la salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_15.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_15.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_16.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_16.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variaciones de la arquitectura LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos de codificador-decodificador realizan tareas de secuencia a secuencia, tales como la traducción, donde la secuencia de entrada y la secuencia de salida pueden tener diferentes longitudes.\n",
    "\n",
    "Se puede entrenar este tipo de modelos para realizar generación de tareas de texto (BART y T5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_17.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_17.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos de solo codificador son modelos de secuencia a secuencia (la secuencia de entrada y la secuencia de salida tienen la misma longitud) utilizados para extraer información relevante de la entrada y generar incrustaciones para otras tareas posteriores como clasificación o análisis (BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_18.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_18.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos sólo con decodificador son algunos de los más utilizados en la actualidad. Son similares a los modelos codificador-decodificador, pero la información de entrada no está codificada explícitamente. Están entrenados para predecir el siguiente token en una secuencia dada la fichas anteriores y pueden generalizar la mayoría de las tareas (GPT y LlaMA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_19.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_19.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El trabajo para desarrollar y mejorar el mensaje se conoce como mensaje de ingeniería (prompt engineering).\n",
    "\n",
    "Una estrategia poderosa para lograr que el modelo produzca mejores resultados es incluir ejemplos de la tarea que desea que realice el modelo dentro del mensaje. Proporcionar ejemplos dentro de la ventana de contexto se le llama aprendizaje en contexto.\n",
    "\n",
    "Entonces, el aprendizaje en contexto puede ayudar a los LLM a aprender más sobre la tarea que se solicita al incluir ejemplos o datos adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_20.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_20.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_21.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_21.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_22.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_22.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden diseñar indicaciones para alentar al modelo a aprender ejemplos.\n",
    "\n",
    "Si bien los modelos más grandes son buenos para realizar inferencias de disparo cero.Por ejemplo, los modelos más pequeños pueden beneficiarse de un solo disparo o de pocos disparos de inferencia que incluyen ejemplos del comportamiento deseado.\n",
    "\n",
    "Sin embargo, se debe recordar la ventana de contexto, ya que hay un límite en la cantidad de aprendizaje en contexto que se puede pasar al modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ajuste fino es un método que se puede utilizar para mejorar el rendimiento de un modelo LLM existente para un caso de uso específico.\n",
    "\n",
    "En primer lugar, hay un paso previo a la capacitación en el que el LLM se capacita utilizando grandes cantidades de datos textuales no estructurados a través de autoaprendizaje supervisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_23.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_23.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de eso, se utiliza el ajuste fino (fine-tuning): Es un proceso de aprendizaje supervisado donde se utiliza un conjunto de datos de ejemplos etiquetados para actualizar las ponderaciones del LLM.\n",
    "\n",
    "El proceso de ajuste extiende el entrenamiento del modelo para mejorar su capacidad de generar buenas terminaciones para una tarea específica.\n",
    "\n",
    "Una estrategia se conoce como ajuste fino de instrucciones: Entrena el modelo utilizando ejemplos que demuestren cómo debe responder a una instrucción específica (los ejemplos etiquetados son pares de sugerencias para completar).\n",
    "\n",
    "Curiosamente, se pueden lograr buenos resultados con relativamente pocos ejemplos: A menudo, sólo 500-1000 ejemplos pueden dar como resultado un buen rendimiento (en contraste a los miles de millones de textos que el modelo vio durante el entrenamiento previo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_24.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_24.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_25.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_25.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, existe una desventaja potencial al realizar ajustes en una sola tarea: El proceso puede conducir a un fenómeno llamado olvido catastrófico.\n",
    "\n",
    "Esto sucede porque el proceso de ajuste completo modifica los pesos del LLM original.\n",
    "\n",
    "Si bien esto conduce a un gran rendimiento en una sola tarea de ajuste fino, puede degradar el rendimiento en otras tareas.\n",
    "\n",
    "Esto se puede evitar con:\n",
    "\n",
    "• Ajuste de instrucciones multitarea: Ajuste de múltiples tareas al mismo tiempo.\n",
    "\n",
    "• Ajuste eficiente de parámetros (PEFT): Un conjunto de técnicas que preserva los pesos del entrenamiento LLM original, es decir, solo una pequeña cantidad de adaptadores específicos de capas y parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_26.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_26.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pleno ajuste, el peso de cada modelo se actualiza durante el proceso de aprendizaje supervisado.\n",
    "\n",
    "En PEFT, los métodos sólo actualizan un pequeño subconjunto de parámetros.\n",
    "\n",
    "Algunas técnicas congelan la mayoría de los pesos del modelo y se centran en el ajuste fino un subconjunto de parámetros de modelo existentes, por ejemplo, capas particulares o componentes.\n",
    "\n",
    "Otras técnicas no tocan los pesos del modelo original en absoluto, y en su lugar se agrega una pequeña cantidad de nuevos parámetros o capas y ajusta solo los nuevos componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_27.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_27.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Específicamente, los métodos PEFT son:\n",
    "\n",
    "• Los métodos selectivos son aquellos que afinan solo un subconjunto de parámetros del LLM original con varios enfoques para identificar qué parámetros se desea actualizar. Existe la opción de entrenar solo ciertos componentes del modelo o  capas específicas, o incluso tipos de parámetros individuales.\n",
    "\n",
    "• Un subgrupo llamado métodos de reparametrización que también funciona con los parámetros del LLM original, pero reduciendo el número de parámetros a entrenar mediante la creación de nuevos parámetros de rango bajo mediante transformaciones de los pesos originales de la red. Una técnica comúnmente utilizada de este tipo es la adaptación de bajo rango de modelos de lenguaje grandes (LoRA).\n",
    "\n",
    "• Los métodos aditivos realizan ajustes manteniendo todos los pesos LLM originales congelados e introduciendo nuevos componentes entrenables.\n",
    "\n",
    "• Los métodos adaptadores añaden nuevas capas entrenables a la arquitectura del modelo, normalmente dentro de los componentes codificadores o decodificadores después de las capas de atención o de retroalimentación.\n",
    "\n",
    "• Los métodos de avisos suaves, por otro lado, mantienen la arquitectura del modelo fija y congelada, y centrarse en manipular la entrada para lograr un mejor rendimiento. Esto se puede hacer añadiendo parámetros entrenables a las incorporaciones rápidas o mantener la entrada fija y volver a entrenar el pesas incrustadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las partes codificadoras y decodificadoras del transformador hay dos tipos de redes neuronales: redes de autoatención y feed-forward.\n",
    "\n",
    "Los pesos de estas redes se aprenden durante el entrenamiento previo.\n",
    "\n",
    "LoRA congela todos los parámetros del modelo original y luego inyecta un par de matrices de descomposición de rangos junto con los pesos originales.\n",
    "\n",
    "Las dimensiones de las matrices más pequeñas se establecen de manera que su producto sea una matriz con las mismas dimensiones que los pesos que se están modificando.\n",
    "\n",
    "Los investigadores han descubierto que aplicar LoRA solo a la autoatención de las capas del modelo suelen ser suficientes para realizar un ajuste preciso de una tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_28.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_28.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de inferencia, las dos matrices de bajo rango se multiplican para obtener una matriz con las mismas dimensiones que los pesos congelados.\n",
    "\n",
    "Luego se agregan éstos a los pesos originales y se reemplazan en el modelo con estos valores actualizados.\n",
    "\n",
    "Ahora existe un modelo LoRA ajustado que puede llevar a cabo una tarea específica.\n",
    "\n",
    "Hay poco o ningún impacto en la latencia de inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_29.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_29.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ajustar un conjunto diferente de matrices LoRA para diferentes tareas y luego cambiarlos en el momento de la inferencia actualizando los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_30.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_30.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting suave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con un ajuste rápido, se agregan tokens entrenables adicionales al indicarlo y dejarlo en manos del proceso de aprendizaje supervisado para determinar sus valores óptimos.\n",
    "\n",
    "El conjunto de fichas entrenables se denomina indicaciones suaves y se agrega al incrustar vectores que representan el texto de entrada.\n",
    "\n",
    "Los vectores de aviso suave tienen la misma longitud que la incrustación de vectores de las fichas de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_31.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_31.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_32.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_32.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede entrenar un conjunto diferente de indicaciones suaves para cada tarea y luego cambiarlos fácilmente en el momento de la inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_33.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_33.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros de configuración para inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los patios de juegos de LLM generalmente se presentan con algunos controles para ajustar un conjunto de parámetros de configuración para cambiar el comportamiento del LLM.\n",
    "\n",
    "Este conjunto de parámetros de configuración influye en la salida del modelo durante la inferencia.\n",
    "\n",
    "Los parámetros de configuración se invocan en el momento de la inferencia y le brindan control sobre las cosas.\n",
    "\n",
    "Algunos parámetros son el número máximo de tokens en el finalización u otros que influyen en la creatividad del resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_34.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_34.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max new tokens se utiliza para limitar el número de tokens que el modelo generará.\n",
    "\n",
    "Es el máximo de tokens nuevos, no una cantidad fija de tokens nuevos generado (si se alcanza una condición de parada, el modelo finaliza la generación).\n",
    "\n",
    "La salida de la capa softmax del transformador es una probabilidad de distribución en todo el diccionario de palabras que el modelo usa.\n",
    "\n",
    "La forma más simple de predicción de la siguiente palabra, donde el modelo elige siempre la palabra con mayor probabilidad, se llama muestreo codicioso o decodificación codiciosa.\n",
    "\n",
    "Este método puede funcionar muy bien para generaciones cortas, pero es susceptible a palabras repetidas o secuencias repetidas de palabras.\n",
    "\n",
    "Si se quiere generar texto que sea más natural, más creativo y que evite repetir palabras, es necesario utilizar otros métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_35.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_35.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El muestreo aleatorio es una forma de introducir cierta variabilidad.\n",
    "\n",
    "Con un muestreo aleatorio, el modelo elige una palabra de salida en aleatorio utilizando la distribución de probabilidad para ponderar la selección.\n",
    "\n",
    "Al utilizar esta técnica, la probabilidad de que las palabras se repitan es reducida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_36.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_36.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top K y Top P son técnicas de muestreo que se pueden utilizar para limitar el muestreo aleatorio y aumentar la probabilidad de un resultado razonable.\n",
    "\n",
    "Top K limita las opciones y al mismo tiempo permite cierta variabilidad especificando que el modelo elige sólo entre los K tokens con mayor probabilidad utilizando la técnica de muestreo aleatorio.\n",
    "\n",
    "Una configuración de decodificación voraz es equivalente a Top K=1.\n",
    "\n",
    "Alternativamente, Top P limita el muestreo aleatorio a tokens cuyas probabilidades combinadas no excedan P.\n",
    "\n",
    "Top P no se utiliza a menos que establezca el valor del parámetro Top P en algo distinto al valor predeterminado de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_37.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_37.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se especifican ambos parámetros (K y P), Top K se aplica primero, y cualquier token por debajo del límite establecido por Top K se considera que tiene una probabilidad de cero cuando se calcula Top P.\n",
    "\n",
    "Otro parámetro que se utiliza para controlar la aleatoriedad de la salida del modelo es la temperatura.\n",
    "\n",
    "Este parámetro influye en la forma de la distribución de probabilidad de la capa de salida softmax del diccionario de palabras utilizado por el modelo.\n",
    "\n",
    "En general, cuanto mayor es la temperatura, mayor es la aleatoriedad y cuanto menor es la temperatura, menor es la aleatoriedad.\n",
    "\n",
    "El valor de temperatura es un factor de escala que se aplica a la distribución de probabilidad de la capa de salida softmax y afecta la forma de la distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <img src=\"./figures/LLM_38.png\" format=\"png\">\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./figures/LLM_38.png\" format=\"png\">\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• https://www.coursera.org/learn/generative-ai-with-llms\n",
    "\n",
    "• https://developers.google.com/machine-learning/resources/intro-llms\n",
    "\n",
    "• https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/\n",
    "\n",
    "• https://stanford-cs324.github.io/winter2022/lectures\n",
    "\n",
    "• https://docs.cohere.com/docs/intro-large-language-models\n",
    "\n",
    "• https://towardsdatascience.com/a-practical-introduction-to-llms-65194dda1148\n",
    "\n",
    "• https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
    "\n",
    "• https://www.linkedin.com/pulse/transformer-architectures-dummies-part-2-decoder-only-qi6vc/\n",
    "\n",
    "• https://www.baeldung.com/cs/large-language-models#general_architecture_of_llms\n",
    "\n",
    "• https://www.ibm.com/docs/en/watsonx-as-a-service?topic=lab-model-parameters-prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir Jupyter Notebook a Fichero Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LLM_notebook.ipynb to script\n",
      "[NbConvertApp] Writing 29670 bytes to LLM_notebook.py\n"
     ]
    }
   ],
   "source": [
    "! python .convert_notebook_to_script.py --input LLM_notebook.ipynb --output LLM_notebook.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
