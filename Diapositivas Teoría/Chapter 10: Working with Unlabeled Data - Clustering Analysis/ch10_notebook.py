# coding: utf-8


from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import numpy as np
from matplotlib import cm
from sklearn.metrics import silhouette_samples
import pandas as pd
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import dendrogram
# from scipy.cluster.hierarchy import set_link_color_palette
from packaging import version
from sklearn.cluster import AgglomerativeClustering 
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN

# # Cap√≠tulo 10: Trabajar con datos sin etiquetar: an√°lisis de agrupaci√≥n

# # √çndice

# - [Introducci√≥n](#introducci√≥n)
# - [Agrupar objetos por similitud usando k-means](#agrupar-objetos-por-similitud-usando-k-means)
# - [Organizar cl√∫steres como un √°rbol jer√°rquico](#organizar-cl√∫steres-como-un-√°rbol-jer√°rquico)
# - [Localizaci√≥n de regiones de alta densidad mediante DBSCAN](#localizaci√≥n-de-regiones-de-alta-densidad-mediante-dbscan)
# - [Convertir Jupyter Notebook a Fichero Python](#convertir-jupyter-notebook-a-fichero-python)




# Image(filename='./figures/01_01.png', width=500)

# display(HTML("""
# <div style="display: flex; justify-content: center;">
#     <img src="./figures/01_01.png" width="500" height="300" format="png">
# </div>
# """))


# ## Introducci√≥n

# Las t√©cnicas de aprendizaje supervisado crean modelos de aprendizaje autom√°tico utilizando datos cuya respuesta ya se conoc√≠a. Por ejemplo, en clasificaci√≥n, las etiquetas de clase est√°n disponibles en capacitaci√≥n de datos.
# 
# Las t√©cnicas de aprendizaje no supervisadas construyen el aprendizaje autom√°tico modelos que nos permiten descubrir estructuras ocultas en los datos donde no sabemos la respuesta correcta de antemano. Por ejemplo, en la agrupaci√≥n, el modelo intenta encontrar una relaci√≥n natural agrupar datos para que los elementos del mismo grupo sean m√°s similares a entre s√≠ que con aquellos de diferentes grupos.

# ## Agrupar objetos por similitud usando k-means

# El algoritmo k-means es uno de los algoritmos de agrupamiento m√°s populares, que se utilizan ampliamente tanto en el mundo acad√©mico como en industria.
# 
# Intenta encontrar grupos de objetos similares que est√©n m√°s relacionados entre s√≠, excepto a objetos de otros grupos.
# 
# Ejemplos de aplicaciones de clustering orientadas a los negocios incluyen la agrupaci√≥n de documentos, m√∫sica y pel√≠culas por diferentes temas o encontrar clientes que compartan intereses similares basados sobre comportamientos de compra comunes como base para la recomendaci√≥n.
# 
# El algoritmo k-means es extremadamente f√°cil de implementar y computacionalmente muy eficiente en comparaci√≥n con otros algoritmos de agrupaci√≥n.
# 
# El algoritmo k-means pertenece a la categor√≠a de agrupamiento basado en prototipos. Existen otras categor√≠as de agrupamiento, como el jer√°rquico y el agrupamiento basado en densidad.
# 
# La agrupaci√≥n basada en prototipos significa que cada grupo est√° representado por un prototipo, que suele ser el centroide (promedio) de puntos similares con caracter√≠sticas continuas, o el medoide (el punto m√°s representativo o que minimiza la distancia a todos los dem√°s puntos que pertenecen a un grupo particular) en el caso de los rasgos categ√≥ricos.
# 
# K-means es muy bueno para identificar grupos con una forma esf√©rica. Esto se debe a que el algoritmo minimiza la suma de la distancia de los cuadrados entre los puntos de datos y el centroide del grupo. En un espacio euclidiano, esta minimizaci√≥n tiende a formar grupos que son esf√©ricos alrededor de sus centroides.
# 
# Uno de los inconvenientes de este algoritmo de agrupamiento es que tenemos para especificar el n√∫mero de grupos, k, a priori. Una inapropiada elecci√≥n de k puede dar como resultado un rendimiento de agrupaci√≥n deficiente.
# 
# El m√©todo del codo y los gr√°ficos de silueta son t√©cnicas √∫tiles para evaluar la calidad de una agrupaci√≥n para ayudarnos a determinar la n√∫mero √≥ptimo de grupos, k.
# 
# Aunque la agrupaci√≥n de k-means se puede aplicar a datos en dimensiones superiores, analizaremos en siguientes ejemplos que utilizan un conjunto de datos bidimensional simple con fines de visualizaci√≥n.



X, y = make_blobs(n_samples=150, 
                  n_features=2, 
                  centers=3, 
                  cluster_std=0.5, 
                  shuffle=True, 
                  random_state=0)
plt.scatter(X[:, 0], 
            X[:, 1], 
            c='white', 
            marker='o', 
            edgecolor='black', 
            s=50)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid()
plt.tight_layout()
plt.show()


# El objetivo es agrupar los ejemplos seg√∫n similitud entre las caracter√≠sticas utilizando el algoritmo k-means, como se resume en los siguientes pasos:
# 
# 1. Elegir aleatoriamente k centroides de los ejemplos como grupo inicial.
# 
# 2. Asignar cada ejemplo al centroide m√°s cercano (m√°s similar), ùúá(ùëó), ùëó ‚àà {1, ‚Ä¶ , ùëò}.
# 
# 3. Mover los centroides al centro de los ejemplos que fueron asignados.
# 
# 4. Repetir los pasos 2 y 3 hasta que las asignaciones de cl√∫ster no cambien o la tolerancia de cambio definida por el usuario o el n√∫mero m√°ximo de iteraciones haya sido alcanzado.
# 
# ¬øC√≥mo medimos la similitud entre objetos?
# 
# Podemos definir la similitud como lo opuesto a la distancia, y un t√©rmino com√∫nmente utilizado para agrupar ejemplos con caracter√≠sticas continuas es la distancia euclidiana al cuadrado entre dos puntos, x e y, en un espacio m-dimensional:
# 
# Nota: En esta ecuaci√≥n, el √≠ndice j se refiere a la j-√©sima dimensi√≥n (columna de caracter√≠sticas) del entradas de ejemplo, x e y.
# 
# IMAGEN 10_01
# 
# Con base en esta m√©trica de distancia euclidiana, podemos describir el algoritmo k-means como un problema de optimizaci√≥n simple. Un enfoque iterativo para minimizar la suma de cuadrados dentro del grupo de errores (SSE) o tambi√©n llamado inercia de cl√∫ster:
# 
# IMAGEN 10_02
# 
# Establecemos el n√∫mero de grupos deseados en 3 (tener que especificar el n√∫mero de grupos a priori es una de las limitaciones de k-means).
# 
# Configuramos n_init=10 para ejecutar los algoritmos de agrupamiento de k-means 10 veces de forma independiente, con diferentes centroides aleatorios para elegir el modelo final como el que tiene el SSE m√°s bajo.
# 
# A trav√©s del par√°metro max_iter, especificamos el n√∫mero m√°ximo de iteraciones para cada ejecuci√≥n (aqu√≠, 300). Hay que tener en cuenta que la implementaci√≥n de k-means en scikit-learn se detiene antes de que el tiempo converja y antes de alcanzar el n√∫mero m√°ximo de iteraciones.
# 
# Sin embargo, es posible que k-means no alcance la convergencia para una ejecuci√≥n en particular, lo que puede ser problem√°tico (computacionalmente costoso) si elegimos valores relativamente grandes para max_iter. Una forma de abordar los problemas de convergencia es elegir valores mayores para tol, que es un par√°metro que controla la tolerancia con respecto a los cambios en el SSE dentro del cl√∫ster para declarar convergencia. En el c√≥digo anterior, elegimos una tolerancia de 1e-04 (=0,0001).



km = KMeans(n_clusters=3, 
            init='random', 
            n_init=10, 
            max_iter=300,
            tol=1e-04,
            random_state=0)
y_km = km.fit_predict(X)


# Escalado de funciones: Cuando aplicamos k-means a datos del mundo real usando una ecuaci√≥n euclidiana m√©trica de distancia, queremos asegurarnos de que las caracter√≠sticas se midan en la misma escala y aplicar la estandarizaci√≥n de puntuaci√≥n z o min-max escalando si es necesario.



plt.scatter(X[y_km == 0, 0],
            X[y_km == 0, 1],
            s=50, c='lightgreen',
            marker='s', edgecolor='black',
            label='Cluster 1')
plt.scatter(X[y_km == 1, 0],
            X[y_km == 1, 1],
            s=50, c='orange',
            marker='o', edgecolor='black',
            label='Cluster 2')
plt.scatter(X[y_km == 2, 0],
            X[y_km == 2, 1],
            s=50, c='lightblue',
            marker='v', edgecolor='black',
            label='Cluster 3')
plt.scatter(km.cluster_centers_[:, 0],
            km.cluster_centers_[:, 1],
            s=250, marker='*',
            c='red', edgecolor='black',
            label='Centroids')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(scatterpoints=1)
plt.grid()
plt.tight_layout()
plt.show()


# Tenemos el inconveniente de tener que especificar el n√∫mero de clusters, k, a priori. El n√∫mero de clusters a elegir puede no ser siempre el mismo en aplicaciones del mundo real, especialmente si estamos trabajando con un conjunto de datos de dimensiones superiores que no se pueden visualizar.
# 
# Adem√°s, el algoritmo k-means utiliza una semilla aleatoria para colocar centroides, que a veces pueden resultar en cl√∫steres defectuosos, lentos o convergentes si los centroides iniciales se eligen mal. Una forma de abordar este problema es ejecutar el algoritmo k-means varias veces en un conjunto de datos y elegir el modelo de mejor rendimiento en t√©rminos de ESS. Otra estrategia es colocar los centroides iniciales lejos unos de otros a trav√©s del algoritmo k-means++, que conduce a resultados mejores y m√°s consistentes que las cl√°sicas k-means.
# 
# La inicializaci√≥n en k-means++ se puede resumir de la siguiente manera:
# 
# IMAGEN 10_03
# 
# El clustering duro describe una familia de algoritmos donde cada ejemplo en un conjunto de datos se asigna exactamente a un grupo, como en los algoritmos k-means y k-means++ que analizamos anteriormente en este cap√≠tulo.
# 
# Por el contrario, los algoritmos para clustering suave (a veces tambi√©n llamados de clustering difuso) asignan un ejemplo a uno o m√°s clusters. Un ejemplo de agrupamiento suave es el algoritmo difuso de medias C (FCM), tambi√©n llamados k-means suaves o k-means difuso.
# 
# Uno de los principales desaf√≠os del aprendizaje no supervisado es que no se sabe la respuesta definitiva. No tenemos etiquetas de clase reales en nuestro conjunto de datos que nos permitan evaluar el desempe√±o del modelo.
# 
# Por lo tanto, para cuantificar la calidad de la agrupaci√≥n, necesitamos utilizar m√©tricas intr√≠nsecas, como la SSE (inercia) dentro del cl√∫ster, para comparar el rendimiento de diferentes agrupaciones de modelos de k-means. En scikit-learn ya se puede acceder a trav√©s del atributo inertia_ despu√©s del ajuste un modelo KMeans.



print(f'Distortion: {km.inertia_:.2f}')


# Basado en el SSE dentro del cl√∫ster, podemos usar una herramienta gr√°fica, el llamado m√©todo del codo, para estimar el n√∫mero √≥ptimo de grupos, k, para una tarea determinada.
# 
# Podemos decir que si k aumenta, la inercia disminuir√°. Esto es porque los ejemplos estar√°n m√°s cerca de los centroides que se han asignado.
# 
# La idea detr√°s del m√©todo del codo es identificar el valor de k donde la distorsi√≥n comienza a aumentar m√°s r√°pidamente.



distortions = []
for i in range(1, 11):
    km = KMeans(n_clusters=i, 
                init='k-means++', 
                n_init=10, 
                max_iter=300, 
                random_state=0)
    km.fit(X)
    distortions.append(km.inertia_)
plt.plot(range(1, 11), distortions, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Distortion')
plt.tight_layout()
plt.show()


# Otra m√©trica intr√≠nseca para evaluar la calidad de un clustering es el an√°lisis de silueta, que tambi√©n se puede aplicar a la agrupaci√≥n de algoritmos distintos de k-means.
# 
# El an√°lisis de silueta se puede utilizar como herramienta gr√°fica para trazar una medida de qu√© tan estrechamente agrupados est√°n los ejemplos en los grupos (mida qu√© tan similar es un objeto a su propio grupo en comparaci√≥n con otros grupos).
# 
# Calcular el coeficiente de silueta de un solo ejemplo en nuestro conjunto de datos, podemos aplicar los siguientes tres pasos:
# 
# IMAGEN 10_04
# 
# Puntuaci√≥n cercana a 1:
# 
# ‚Ä¢ Un valor cercano a 1 indica que el punto de datos est√° bien agrupado dentro de su propio grupo y est√° claramente separado de otros grupos.
# 
# ‚Ä¢ Esto sugiere que el punto de datos es similar a otros puntos en su grupo y diferente de puntos en grupos vecinos. Es se√±al de que el c√∫mulo es compacto y bien definido.
# 
# Puntuaci√≥n de 0:
# 
# ‚Ä¢ Un valor de 0 indica que el punto de datos est√° en el l√≠mite entre dos grupos.
# 
# ‚Ä¢ Esto significa que el punto de datos est√° equidistante entre su propio grupo y el punto de datos m√°s cercano. En este caso, el punto podr√≠a pertenecer a cualquiera de los grupos, lo que sugiere que los racimos no est√°n bien separados.
# 
# Puntuaci√≥n cercana a -1:
# 
# ‚Ä¢ Un valor cercano a -1 indica que el punto de datos probablemente est√© mal asignado a su grupo actual.
# 
# ‚Ä¢ Esto sugiere que el punto de datos es m√°s similar a un grupo vecino que al cluster al que ha sido asignado. Es una se√±al de que el cluster est√° borroso o que los datos del punto podr√≠an estar en el grupo equivocado.
# 
# El coeficiente de silueta est√° disponible como silhouette_samples del m√≥dulo m√©trico scikit-learn.
# 
# La funci√≥n silhouette_scores calcula el promedio coeficiente de silueta en todos los ejemplos, que es equivalente a numpy.mean (silhouette_samples (...)).




km = KMeans(n_clusters=3, 
            init='k-means++', 
            n_init=10, 
            max_iter=300,
            tol=1e-04,
            random_state=0)
y_km = km.fit_predict(X)

cluster_labels = np.unique(y_km)
n_clusters = cluster_labels.shape[0]
silhouette_vals = silhouette_samples(
    X, y_km, metric='euclidean'
)
y_ax_lower, y_ax_upper = 0, 0
yticks = []
for i, c in enumerate(cluster_labels):
    c_silhouette_vals = silhouette_vals[y_km == c]
    c_silhouette_vals.sort()
    y_ax_upper += len(c_silhouette_vals)
    color = cm.jet(float(i) / n_clusters)
    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, 
             edgecolor='none', color=color)
    yticks.append((y_ax_lower + y_ax_upper) / 2.)
    y_ax_lower += len(c_silhouette_vals)
    
silhouette_avg = np.mean(silhouette_vals)
plt.axvline(silhouette_avg, 
            color="red", 
            linestyle="--") 
plt.yticks(yticks, cluster_labels + 1)
plt.ylabel('Cluster')
plt.xlabel('Silhouette coefficient')
plt.tight_layout()
plt.show()


# Los coeficientes de silueta no est√°n cerca de 0 y est√°n aproximadamente igualmente alejados del promedio de puntuaci√≥n de silueta, que es, en este caso, un indicador de una buena agrupaci√≥n. Adem√°s, para resumir la bondad de nuestra agrupaci√≥n, agregamos el coeficiente de silueta promedio a la gr√°fica (l√≠nea de puntos).
# 
# Ejemplo de una mala agrupaci√≥n:



km = KMeans(n_clusters=2,
            init='k-means++',
            n_init=10,
            max_iter=300,
            tol=1e-04,
            random_state=0)
y_km = km.fit_predict(X)
plt.scatter(X[y_km == 0, 0],
            X[y_km == 0, 1],
            s=50,
            c='lightgreen',
            edgecolor='black',
            marker='s',
            label='Cluster 1')
plt.scatter(X[y_km == 1, 0],
            X[y_km == 1, 1],
            s=50,
            c='orange',
            edgecolor='black',
            marker='o',
            label='Cluster 2')
plt.scatter(km.cluster_centers_[:, 0], 
            km.cluster_centers_[:, 1],
            s=250, 
            marker='*', 
            c='red', 
            label='Centroids')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()




cluster_labels = np.unique(y_km)
n_clusters = cluster_labels.shape[0]
silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')
y_ax_lower, y_ax_upper = 0, 0
yticks = []
for i, c in enumerate(cluster_labels):
    c_silhouette_vals = silhouette_vals[y_km == c]
    c_silhouette_vals.sort()
    y_ax_upper += len(c_silhouette_vals)
    color = cm.jet(float(i) / n_clusters)
    plt.barh(range(y_ax_lower, y_ax_upper), 
             c_silhouette_vals, 
             height=1.0, 
             edgecolor='none', 
             color=color)
    yticks.append((y_ax_lower + y_ax_upper) / 2.)
    y_ax_lower += len(c_silhouette_vals)
silhouette_avg = np.mean(silhouette_vals)
plt.axvline(silhouette_avg, color="red", linestyle="--") 
plt.yticks(yticks, cluster_labels + 1)
plt.ylabel('Cluster')
plt.xlabel('Silhouette coefficient')
plt.tight_layout()
plt.show()


# Las siluetas son visiblemente diferentes en cuanto a longitudes y anchos, lo que es evidencia de un agrupamiento relativamente malo o sub√≥ptimo.

# ## Organizar cl√∫steres como un √°rbol jer√°rquico

# Examinaremos un enfoque alternativo al clustering basado en prototipos: agrupamiento jer√°rquico. Organiza los datos en una estructura jer√°rquica, similar a un √°rbol, donde cada nivel de la jerarqu√≠a representa una agrupaci√≥n diferente de los datos. Esta estructura permite observar c√≥mo se agrupan los datos en diferentes niveles de semejanza.
# 
# Una ventaja del algoritmo de agrupamiento jer√°rquico es que permite trazar dendrogramas (visualizaciones de una jerarqu√≠a binaria de agrupamiento), que puede ayudar con la interpretaci√≥n de los resultados.
# 
# Otra ventaja de este enfoque jer√°rquico es que no necesitamos para especificar el n√∫mero de grupos por adelantado.
# 
# Los dos enfoques principales para la agrupaci√≥n jer√°rquica son agrupamiento jer√°rquico aglomerativo y divisivo.
# 
# En el agrupamiento jer√°rquico divisivo, comenzamos con un grupo que abarca el conjunto de datos completo, y dividimos iterativamente el agruparse en grupos m√°s peque√±os hasta que cada grupo solo contenga un ejemplo.
# 
# En el agrupamiento jer√°rquico aglomerativo, tomamos lo opuesto. Comenzamos con cada ejemplo como un grupo individual y fusionamos los pares de clusters m√°s cercanos hasta que solo quede un cluster.
# 
# Los dos algoritmos est√°ndar para la jerarqu√≠a aglomerativa y la agrupaci√≥n son el enlace simple y el enlace completo.
# 
# Utilizando un enlace simple, calculamos las distancias entre los miembros m√°s similares (o m√°s cercanos) para cada par de grupos y fusionar los dos grupos para los cuales la distancia entre los los miembros m√°s cercanos son los m√°s peque√±os.
# 
# El enfoque de vinculaci√≥n completa es similar al de vinculaci√≥n √∫nica pero, en lugar de comparar los miembros m√°s cercanos en cada par de grupos, comparamos los miembros m√°s diferentes (o m√°s lejanos) con realizar la fusi√≥n.
# 
# IMAGEN 10_05
# 
# Nos centraremos en la agrupaci√≥n aglomerativa utilizando el sistema completo de enfoques de vinculaci√≥n. Es un procedimiento iterativo que se puede resumir en los siguiente pasos:
# 
# 1. Calcular una matriz de distancias por pares de todos los ejemplos.
# 
# 2. Representar cada punto de datos como un grupo √∫nico.
# 
# 3. Fusionar los dos grupos m√°s cercanos seg√∫n la distancia entre los miembros m√°s diferentes (distantes).
# 
# 4. Actualizar la matriz de vinculaci√≥n del cl√∫ster.
# 
# 5. Repetir los pasos 2 a 4 hasta que quede un solo grupo.



np.random.seed(123)
variables = ['X', 'Y', 'Z']
labels = ['ID_0', 'ID_1', 'ID_2', 'ID_3', 'ID_4']
X = np.random.random_sample([5, 3])*10
df = pd.DataFrame(X, columns=variables, index=labels)
df


# Calcular la matriz de distancias de todos los ejemplos equivale a calcular la distancia euclidiana entre cada par de ejemplos de entrada en nuestro conjunto de datos.



row_dist = pd.DataFrame(squareform(
                        pdist(df, metric='euclidean')),
                        columns=labels,
                        index=labels)
row_dist


# La matriz de vinculaci√≥n consta de varias filas donde cada fila representa una fusi√≥n. Las primeras dos columnas denotan los miembros m√°s diferentes en cada grupo, y la tercera columna informa la distancia entre esos miembros. La √∫ltima columna devuelve el recuento de miembros de cada grupo.
# 
# IMAGEN 10_06



row_clusters = linkage(df.values, 
                       method='complete', 
                       metric='euclidean')
pd.DataFrame(row_clusters,
             columns=['row label 1', 
                      'row label 2',
                      'distance', 
                      'no. of items in clust.'],
             index=[f'cluster {(i + 1)}' for i in 
                    range(row_clusters.shape[0])])


# Ahora que hemos calculado la matriz de vinculaci√≥n, podemos visualizar los resultados en forma de dendrograma, donde la altura de las uniones indica la distancia o disimilitud entre los cl√∫steres fusionados



# make dendrogram black (part 1/2)
# set_link_color_palette(['black'])
row_dendr = dendrogram(row_clusters, 
                       labels=labels,
                       # make dendrogram black (part 2/2)
                       # color_threshold=np.inf
                       )
plt.tight_layout()
plt.ylabel('Euclidean distance')
plt.show()


# En aplicaciones pr√°cticas, los dendrogramas de agrupamiento jer√°rquico se utilizan a menudo en combinaci√≥n con un mapa de calor, que nos permite representar los valores individuales en la matriz o matriz de datos que contiene nuestros ejemplos de entrenamiento con un c√≥digo de color.



fig = plt.figure(figsize=(8, 8), facecolor='white')
axd = fig.add_axes([0.09, 0.1, 0.2, 0.6])
row_dendr = dendrogram(row_clusters, 
                       orientation='left')
# note: for matplotlib < v1.5.1, please use
# orientation='right'
df_rowclust = df.iloc[row_dendr['leaves'][::-1]]
axd.set_xticks([])
axd.set_yticks([])
for i in axd.spines.values():
    i.set_visible(False)
axm = fig.add_axes([0.23, 0.1, 0.6, 0.6])
cax = axm.matshow(df_rowclust, interpolation='nearest', cmap='hot_r')
fig.colorbar(cax)
axm.set_xticklabels([''] + list(df_rowclust.columns))
axm.set_yticklabels([''] + list(df_rowclust.index))
plt.show()


# Usando scikit-learn:



ac = AgglomerativeClustering(n_clusters=3,
                             metric='euclidean',
                             linkage='complete')
labels = ac.fit_predict(X)
print(f'Cluster labels: {labels}')




ac = AgglomerativeClustering(n_clusters=2,
                             metric='euclidean',
                             linkage='complete')
labels = ac.fit_predict(X)
print(f'Cluster labels: {labels}')


# ## Localizaci√≥n de regiones de alta densidad mediante DBSCAN

# Vamos a incluir un enfoque m√°s para la agrupaci√≥n: agrupaci√≥n espacial de aplicaciones con ruido basada en densidad (DBSCAN).
# 
# Como su nombre lo indica, la agrupaci√≥n basada en densidad asigna etiquetas basadas en regiones densas de puntos.
# 
# En DBSCAN, la noci√≥n de densidad se define como el n√∫mero de puntos dentro de un radio espec√≠fico, ùúÄ.
# 
# Seg√∫n el algoritmo DBSCAN, se asigna una etiqueta especial a cada ejemplo (punto de datos) utilizando los siguientes criterios:
# 
# ‚Ä¢ Un punto se considera un punto central si al menos un n√∫mero espec√≠fico (MinPts) de puntos vecinos se encuentran dentro del radio especificado, ùúÄ.
# 
# ‚Ä¢ Un punto fronterizo es un punto que tiene menos vecinos que MinPts dentro de ùúÄ pero se encuentra dentro del radio ùúÄ de un punto central.
# 
# ‚Ä¢ Se consideran todos los dem√°s puntos que no son centrales ni puntos de ruido fronterizos.
# 
# Despu√©s de etiquetar los puntos como n√∫cleo, borde o ruido, el algoritmo DBSCAN se puede resumir en dos simples pasos:
# 
# 1. Forma un grupo separado para cada punto central o grupo conectado de n√∫cleos (los puntos centrales est√°n conectados si no est√°n m√°s lejos que ùúÄ.)
# 
# 2. Asigna cada punto fronterizo al grupo de su punto central correspondiente.
# 
# IMAGEN 10_07



X, y = make_moons(n_samples=200, 
                  noise=0.05, 
                  random_state=0)
plt.scatter(X[:, 0], X[:, 1])
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.tight_layout()
plt.show()




f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))
km = KMeans(n_clusters=2, random_state=0)
y_km = km.fit_predict(X)
ax1.scatter(X[y_km == 0, 0], 
            X[y_km == 0, 1],
            edgecolor='black',
            c='lightblue', 
            marker='o', 
            s=40, 
            label='cluster 1')
ax1.scatter(X[y_km == 1, 0], 
            X[y_km == 1, 1],
            edgecolor='black',
            c='red', 
            marker='s', 
            s=40, 
            label='cluster 2')
ax1.set_title('K-means clustering')
ax1.set_xlabel('Feature 1')
ax1.set_ylabel('Feature 2')

ac = AgglomerativeClustering(n_clusters=2,
                             metric='euclidean',
                             linkage='complete')
y_ac = ac.fit_predict(X)
ax2.scatter(X[y_ac == 0, 0], 
            X[y_ac == 0, 1], 
            c='lightblue',
            edgecolor='black',
            marker='o', 
            s=40, 
            label='Cluster 1')
ax2.scatter(X[y_ac == 1, 0], 
            X[y_ac == 1, 1], 
            c='red',
            edgecolor='black',
            marker='s', 
            s=40, 
            label='Cluster 2')
ax2.set_title('Agglomerative clustering')
ax2.set_xlabel('Feature 1')
ax2.set_ylabel('Feature 2')
plt.legend()
plt.tight_layout()
plt.show()




db = DBSCAN(eps=0.2, 
            min_samples=5, 
            metric='euclidean')
y_db = db.fit_predict(X)
plt.scatter(X[y_db == 0, 0], 
            X[y_db == 0, 1],
            c='lightblue', 
            marker='o', 
            s=40,
            edgecolor='black', 
            label='Cluster 1')
plt.scatter(X[y_db == 1, 0], 
            X[y_db == 1, 1],
            c='red', 
            marker='s', 
            s=40,
            edgecolor='black', 
            label='Cluster 2')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.tight_layout()
plt.show()


# ## Convertir Jupyter Notebook a Fichero Python




